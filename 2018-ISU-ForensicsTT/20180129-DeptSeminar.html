<!DOCTYPE html>
<html>
  <head>
    <title>CoNNOR: Convolutional Neural Network for Outsole Recognition</title>
    <meta charset="utf-8">
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/csafe.css" type="text/css" />
    <link rel="stylesheet" href="css/csafe-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# CoNNOR: Convolutional Neural Network for Outsole Recognition

---

class: primary





class:primary
## Outline

&lt;br/&gt;
- Defining the Problem
- Identifying features in shoes
- Convolutional Neural Networks
- Future Work

---
class:inverse
&lt;h1&gt;&lt;br/&gt;What is the probability I made this shoeprint?&lt;/h1&gt;

---
class:primary
## What is the probability I made this shoeprint?

1. Define the comparison population
2. Obtain data on the distribution of shoes in the comparison population
3. Identify shoes in the comparison population which could have produced the shoeprint
4. Establish whether I have any matching shoes

???

Comparison population definition could include various size geographic regions, temporal components, interaction with local events - shoes worn by people attending a football game likely differ from shoes worn by people at a formal event. Local areas range from state-level to "People outside Starbucks in CampusTown at 8am on a Monday morning"

---
class:primary
## Comparison Population

&lt;br/&gt;
&gt; .large[Quantifying the frequency of shoes in a local population is an unsolveable problem]&lt;br/&gt; - Leslie Hammer, March 2018


---
class:primary
## Comparison Population
&lt;br/&gt;

- No 100% complete database of all shoes 
    - manufacturer, model, size, tread style, manufacturing molds .small[[Ham89]]
    
- Shoe purchase data vs. frequency of wear

- Local populations may differ wildly .small[[Ben+14]]

&lt;br/&gt;&lt;br/&gt;
.center[&lt;img src = "snow-boots.jpg" width = "50%" style = "vertical-align:middle;float:middle"/&gt;]
&lt;!-- https://pixnio.com/free-images/2017/05/03/2017-05-03-07-35-18-900x456.jpg --&gt;

???

There are too many manufacturers to keep track of, new models are released all the time, custom shoe markets, and knockoff shoes... hard to get a database with everything.

In addition, many of us have shoes that we've purchased but almost never wear... so going off of purchase data (even if that had the geographic resolution needed) is not all that representative

Finally, geographic resolution is nearly impossible. If this pair of shoes was used to commit a crime in Florida, it would be fairly damning to have the same type of shoes in your closet - they aren't "Florida" shoes. If you have this pair of shoes in your closet in Iowa right now, the weight of that evidence is a lot lower, because they're probably fairly common shoes to have. 

---
class:primary
## Comparison Population Shoes

How to collect data from the local population? 

1. Build a low profile scanner that can be placed in a high traffic area

2. Scan shoes of those walking past

3. Create a local-area database of relevant scans

--

### .center[This is an engineering problem]

---
class:primary
## Comparison Population Shoes

Assume a machine exists that can scan shoe outsoles of pedestrians

--

1. Identify relevant features within the scans

--

2. Assess the frequency of similar shoes in the local-area database

--

### .center[These are statistics and machine learning problems]

---
class:primary
## Relevant Features

&lt;br/&gt;
Class Characteristics
- Make, Model, Tread pattern, Size, Type of shoe

- Cannot be used to identify an individual match

- Used for exclusion

---
class:primary
## Relevant Features

&lt;br/&gt;
Use features other than make/model to characterize shoes

- Knockoffs often have very similar tread patterns
- Similar styles have similar tread patterns across brands
- Unknown shoes can still be classified and assessed

&lt;img src = "star_bar_drmartin.png" width = "16%" style = "vertical-align:middle;padding:30px"/&gt;
&lt;img src = "star_bar_vibram.png" width = "15%" style = "vertical-align:middle;padding:30px"/&gt;
&lt;img src = "star_bar_timberland.png" width = "16%" style = "vertical-align:middle;padding:30px"/&gt;

---
class:primary
## Relevant Features

&lt;br/&gt;
Geometric Elements in Shoe Tread
- One type of class characteristic
- Can be used to narrow shoe prints down to make/model where that information exists [GJN13]

| Bowtie | Chevron | Circle |
| ------ | ------- | ------ |
| ![Bowtie examples](class_examples/bowtie_examples.png) | ![Chevron examples](class_examples/chevron_examples.png) | ![Circle examples](class_examples/circle_examples.png) |

| Line | Polygon | Quad |
| ---- | ------- | ---- |
| ![Line examples](class_examples/line_examples.png) | ![Polygon examples](class_examples/polygon_examples.png) | ![Quad examples](class_examples/quad_examples.png) |

| Star | Text | Triangle |
| ---- | ---- | -------- |
| ![Star examples](class_examples/star_examples.png) | ![text examples](class_examples/text_examples.png) | ![Triangle examples](class_examples/triangle_examples.png) |

---
class: inverse
# &lt;br/&gt;Image Analysis and Feature Detection

---
class: primary
## Image Analysis

&lt;br/&gt;
### Goal: Identify geometric tread features in images of shoe outsoles

- Robust to different lighting conditions, rotation, image quality

- Quick to process new images

- Identified features should be explainable to practitioners

--

.center[&lt;img src="https://imgs.xkcd.com/comics/fourier.jpg" width = "40%"/&gt;]

---
class:primary
## Feature Detection

&lt;br/&gt;
#### Classic computer vision feature detection methods: 

- Edge, Corner, Blob, Ridge detection

- Template matching: Hough transforms 
    - line, circle, ellipse detection
    - provide location and orientation

--

.pull-left[
Pros
- No training data necessary
- Relatively simple algorithm
]

--

.pull-right[
Cons
- Not robust (fragile tuning parameters)
- Computationally intensive
- Features lack face validity
]

???

There are some computer vision methods that detect some of the geometric features we've identified. Edge and corner detection are used in image matching, blob detection is used everywhere from biology (identifying cells, nuclei, tumors) to astrophysics (galaxies and stars), and ridge detection is used to identify topographic features in images. More complicated features, such as lines, circles, and ellipses can be detected with Hough transforms, which are a template matching algorithm that's very computationally intensive. 

I spent some time trying to assemble some of these detectors to work with shoe outsole images and found that they were not robust - parameter settings that worked well for one image failed miserably when used on the same model of shoe with a slightly different color sole, the algorithms were slow, and the features they picked out were often not what I would call an edge or a corner - the algorithm was working at a much finer level than I was. As all of our work has to be explained to practitioners eventually, that made these methods sub-optimal. Google, Facebook, and many others have used convolutional neural networks to solve the image recognition problem in the past 10 years, so I moved on to the big guns. 

---
class:primary
## Feature Detection

&lt;br/&gt;
#### Convolutional neural networks: 
- Structure designed to mimic perceptual pathways in the human visual system
- Ubiquitous in modern image recognition tasks

--

.pull-left[

Pros
- Pre-trained networks available for tuning    
.small[AlexNet, VGG16, ResNet, Inception]
- Features are interpretable
- Very fast (after training)

]

--

.pull-right[

Cons
- Requires labeled training data (lots!)
- Computationally intensive to train
- Opaque - parameters are not interpretable

]

---
class:inverse
# &lt;br/&gt;&lt;br/&gt;Convolutional Neural Networks







&lt;!-- Add in information about what the layers are doing: https://www.overleaf.com/project/5bef1722f138c36ed9d572d4 --&gt;
---
class:primary
## CNN Architecture - VGG16

&lt;br/&gt;
&lt;img src="vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

---
class: primary
## Convolutional Layers

&lt;br/&gt;

.moveleft[![Input image and filter](filter.png)]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]
---
class: primary
## Convolutional Layers

&lt;br/&gt;

.moveleft[![Input image and filter](filter1.png)]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

---
class: primary
## Convolutional Layers

&lt;br/&gt;

.moveleft[![Input image and filter](filter2.png)]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]


---
class: primary
## Convolutional Layers

&lt;br/&gt;
For layer `\(\ell\)` and cell `\(ij\)`, with weights `\(\mathbf{w}\)` 

`$$x^\ell_{ij} = \sum_{a=0}^{m-1}\sum_{a=0}^{m-1} w_{ab} y_{(i+a)(j+b)}^{\ell-1}$$`

The  nonlinear activation function `\(\sigma(\cdot)\)` is then applied: `\(y_{ij}^\ell = \sigma(x_{ij}^\ell)\)` 

--

This is "Forward Propagation"

---
class: primary
## Max Pooling Layers

&lt;br/&gt;

.moveleft[![Pooling Layers](maxpooling.png)]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

---
class: primary
## Densely Connected Layers

&lt;br/&gt;

.moveleft[![Dense layers](densely_connected.png)]

---
class: primary
## Dropout Layers

&lt;br/&gt;

.moveleft[![Dropout layers](dropout.png)]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]


---
class: primary
## Fitting Mechanism

&lt;br/&gt;

- Forward Propagation: Input -&gt; Filters -&gt; Pooling -&gt; Result

- Backward Propagation: Errors -&gt; Pooling -&gt; Filters to modify weights
    - Loss function `\(L\)` describing the prediction errors
    - Compute `\(\displaystyle\left(\frac{\partial L}{\partial y_{ij}^\ell}\right)\)` for each cell in the previous layer
    

`$$\left(\frac{\partial L}{\partial w_{ab}}\right) = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} \frac{\partial x^\ell_{ij}}{\partial w_{ab}} = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} y^{\ell-1}_{(i+a)(j+b)}$$`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]
---
class: primary
## Backward Propagation

&lt;br/&gt;

`\(\displaystyle \frac{\partial L}{\partial x^{\ell}_{ij}}\)` is the gradient component:

`$$\frac{\partial L}{\partial x^{\ell}_{ij}} = \frac{\partial L}{\partial y_{ij}^\ell}\frac{\partial y_{ij}^\ell}{\partial x_{ij}^\ell} = \frac{\partial L}{\partial y_{ij}^\ell} \frac{\partial }{\partial x_{ij}^\ell}\left(\sigma(x_{ij}^\ell)\right) = 
 \frac{\partial L}{\partial y_{ij}^\ell} \sigma'(x_{ij}^\ell)$$`

`\(\displaystyle\frac{\partial L}{\partial y_{ij}^\ell}\)` is error at the current layer, so the gradiant can be computed with the derivative of the activation function `\(\sigma(x)\)`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]
---
class: primary
## Backward Propagation

&lt;br/&gt;
To propagate errors to the previous layer, 

`$$\frac{\partial L}{\partial y_{ij}^{\ell -1}} = \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell}\frac{\partial x_{(i-a)(j-b)}^\ell}{\partial y_{ij}^{\ell-1}}
= \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell} \omega_{ab}$$`

&lt;br/&gt;
.center[&lt;img src="vgg16-shoe.png" width = "50%"/&gt;]

.center[&lt;b&gt;13 convolutional layers = a lot of backpropagation&lt;/b&gt;]

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]

---
class: primary
## VGG16

&lt;br/&gt;
- Convolutional base of VGG16 for 256x256x3 input images has 14,714,688 parameters

- Head has 155,090,944 parameters and outputs 1000 different class label predictions, fit using 150,000 images

--

- We have 9 class labels and &lt;25,000 labeled images

- Solution: Use the pre-trained VGG16 base (all of the convolutional layers) and train a new head
    - Reduces the parameter space to 8,391,177 parameters

???

Equivalent of putting a new "brain" on a set of "eyes" that's pre-trained

---
class: inverse
# Fitting CoNNOR: Convolutional Neural Network for Outsole Recognition

---
class:primary
## Acquire Data
&lt;br/&gt;
.moveleft[![Zappos Screenshot showing sole images](zappos.png)]

.move-margin[&lt;br/&gt;&lt;br/&gt;49745 shoe outsole images scraped since June 2018]

---
class:primary
## Label Data

&lt;br/&gt;
.moveleft[.moveleft[![LabelMe](LabelMe1.png)]

- 31,496 shoe images labeled

- 23,898 regions labeled with one or more geometric objects
]
--
.small[.move-margin[
&lt;br/&gt;&lt;br/&gt;Labeling courtesy of 
- Jenny Kim
- Ben Wonderlin
- Holden Jud
- Mya Fisher
- Miranda Tilton
- Charlotte Roiger
- and others
]]

---
class:primary
## Label Data
&lt;br/&gt;
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/unnamed-chunk-5-1.png" alt="Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon." width="99%" /&gt;
&lt;p class="caption"&gt;Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon.&lt;/p&gt;
&lt;/div&gt;

---
class:primary
## Model Specification

&lt;br/&gt;
Multiple classes, multiple labels: "One-hot" encoding

Statistically: 

- Model output: `\((P_1, ..., P_9) \sim Bernoulli(\mathbf{\pi})\)`
    - Each geometric feature assigned a probability
    - An image can be labeled with multiple features

- Output probabilities are not independent
    - Dependencies due to CNN structure
    - Dependencies due to input data
    - Dependencies due to geometry - Polygons vs. Quadrilaterals vs. Lines
    
- Covariance structure is ?

--

.center[There are downsides to black-box models...]

---
class:primary
## Model Training

&lt;br/&gt;
&lt;div style="margin-left:-20px"&gt;
- 256 x 256 pixel images

- Training data (60%):
    - 1x Augmented images (rotation, skew, zoom, crop) to prevent overfitting
    - Class weights used to counteract uneven class sizes
    
- Validation and test data (20% each)

- Fit using the `keras` package in R, which provides a high-level API for the `tensorflow` library .small[[18]]
&lt;/p&gt;

---
class:primary
## Model Training

&lt;br/&gt;
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/training-accuracy-1.png" alt="Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy." width="99%" /&gt;
&lt;p class="caption"&gt;Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.&lt;/p&gt;
&lt;/div&gt;

???

Validation loss levels off after 15 epochs, but hasn't yet begun to increase. Training loss is still decreasing as well. One concern with retraining the head of a CNN is that with relatively little data (e.g. 20 thousand data points instead of 20 million) it is easy to over-fit models; what we see is that this hasn't yet happened for this model. 

---
class:primary
## Evaluating the Model
&lt;!-- Add in model overall AUC --&gt;
&lt;!-- Describe the multi-class version as splitting out model performance by class --&gt; 

&lt;br/&gt;
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/unnamed-chunk-6-1.png" alt="Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class." width="99%" /&gt;
&lt;p class="caption"&gt;Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class.&lt;/p&gt;
&lt;/div&gt;

???

As we discussed previously, this is a multivariate classification problem, in that there are 9 separate output probabilities. I've adapted standard ways of evaluating machine learning model performance to this problem, but it should be noted at this point that the graphs I'm showing here are improvised - ROC curves typically aren't used in classification problems with multiple classes, but once multiple labels are allowed, the problem (somewhat) reduces back to a set of 9 binary classification problems. Here, each ROC curve was generated without considering other features. 

The ROC curves shown here are calculated for each geometric feature without considering the other features. Multi-class ROC curves exist, but do not allow for multi-label classification, so instead separate calculations were performed for each class independently. 

---
class:primary
## Evaluating the Model
&lt;br/&gt;
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/ConfMatrix-1.png" alt="Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes." width="85%" /&gt;
&lt;p class="caption"&gt;Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes.&lt;/p&gt;
&lt;/div&gt;

.move-margin[

&lt;br/&gt;
For multi-label images, only incorrect predictions contribute to off-diagonal probabilities

Probability &gt; Label-wise EER used as the cutoff
&lt;br/&gt;

]

???

Slightly more likely to predict quadrilateral for all labels. Two things are important to note here - first, "reality" and the image labels are different (due to imperfect labeling), and second, what label should be assigned to a specific image is ambiguous. We're working on correcting the mislabeling - some of it is due to the fact that the guidelines for labeling have changed since we started this mid-June. There's a certain amount of ambiguity in the geometric shapes in any case - strictly speaking, any amount of roundness should mean something isn't a quadrilateral, but what is it? Rubber typically doesn't create perfectly sharp corners. 

As with the ROC curves in the previous slide, this image is an adaptation of a confusion matrix, which is usually used to show the performance of a classification algorithm where each item is in only one class. Off-diagonal cells above the diagonal show incorrect classifications, while off-diagonal cells below the diagonal show missed classifications. To modify the confusion matrix for our problem, where an image can be labeled as any number of classes, we have excluded additional class labels when calculating the probability that something is misclassified. So if an image contains both a line and a circle, and the model labels it as containing a circle but not a line, when calculating the probability that a line image is misclassified as a circle, the image will be excluded (since it really should have been classified as a circle). 

---
class:primary
## Definitions matter

&lt;br/&gt;
.moveleft[![Classes get confusing](dc_circle_quad_confusion.png)]

--

.moveleft[![Not everything is labeled correctly](circle_pred_correct.png)]

???

We created a shiny application to see the images and the model's predictions. Blue means that the image had that label, grey means it does not. 

So in the first image, the design is labeled as a quadrilateral (rounded quadrilateral) rather than a circle. We're still debating how to handle these... 

In the second image, both text and quad are labels, but the model also identifies a circle in the text with probability 0.31; that is, the text contains an O. 

We use the shiny application to screen for these problems so that we can correct the labeled training data. We're trying to ensure that the data used to train the model is of very high quality, while not spending millions of dollars to hire workers online to label things. Because we determined the guidelines for labeling the data, labeled the data (or oversaw the labeling), and trained the model ourselves, we have the advantage of knowing the flaws at every point in the process; that means we have the responsibility to fix those flaws where possible. We're not doing inference on the model results at this point (nor planning to use the data we're training the model with during the operational stage) so the data -&gt; model -&gt; fix data loop is less of a validity concern. 

When the model is sufficiently well-calibrated, we can then work with engineers to build the device, collect some initial data, and tweak the model weights with new data that better represents what we'll actually see from the collection equipment. By that point, hopefully we'll also have narrowed down the geometric classification scheme so that categories that are now somewhat fuzzy are more clearly operationalized.

---
class:primary
## Debugging the model

&lt;br/&gt;
- Active area of research: How to debug, interpret, and understand what a CNN is actually doing    
.small[https://distill.pub/2018/building-blocks/]

- Which regions in the image are relevant to the class?

- Which filters are most important for detection of each class?





---
class:primary
## Class Activation Maps

&lt;br/&gt;
.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - DC](heatmaps/heatmap-quad-4-dc-pure-se-navy_product_7270757_color_9.png)
]

---
class:primary
## Class Activation Maps

&lt;br/&gt;
.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - adidas](heatmaps/heatmap-text-1-adidas-kids-adilette-clf-adj-toddler-little-kid-big-kid-black-white_product_8987203_color_151.png)
]

---
class:primary
## Class Activation Maps

&lt;br/&gt;
.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - seychelles](heatmaps/heatmap-text-2-seychelles-slow-down-blush-metallic_product_9017725_color_34700.png)
]


---
class:inverse
# &lt;br/&gt;&lt;br/&gt;&lt;br/&gt;What's Next?

---
class:primary
## Debugging the model
&lt;br/&gt;
- Port the keras-vis Python library to R

    - Activation Maximization maps: generate a new image to maximize filter output activations
        - Show what specific filters are doing and "seeing"
        
    - What parts of an image are most important in activating a specific class?
        - Saliency Maps: plot gradients w.r.t. output
        - Class Activation Maps: plot gradients w.r.t. last convolutional layer (maintains spatial information)
        
&lt;!-- Insert Jason's stuff here --&gt;        
---
class: primary
## Whole-Shoe Predictions
&lt;br/&gt;
- Currently, predictions are for 256 x 256 chunks

- Integrate multiple chunks to provide geometric features with probabilities and relative coordinates

- Add spatial information to further discriminate between shoe models

&lt;!-- --- --&gt;
&lt;!-- class: primary --&gt;
&lt;!-- ## Uncertainty estimation --&gt;
&lt;!-- &lt;br/&gt; --&gt;
&lt;!-- - Confidence/Prediction/Credible intervals for Neural Network predictions and parameters are not common --&gt;
&lt;!--     - Conformal prediction --&gt;
&lt;!--     - [Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) - Bayesian frameworks for CNNs [Shr+18] --&gt;


---
class: primary
## Collect local population data

&lt;br/&gt;
- Build the machine (or rather, have engineers build it...)
- Collect data from (very) local areas
    - date/time stamp
    - side-view pictures of the shoes (where possible)
    - scans of the bottom of the shoe
- Label geometric features on the collected scans
- Run a 2nd stage of model weight optimization with scanned data
- Use this optimized model to analyze newly collected data

---
class: primary
## Local population data

- Use spatial locations + label probabilities as a feature vector

- Database of shoes in local population can be used to assess likelihood of a specific feature set occurring at random
    - Helpful to have error estimates for CNN output probabilities 
        - Bayesian CNNs [Shr+18]
    - Useful to know wear pattern frequency
        - As shoes wear, geometric features become less pronounced
        - Wear patterns can also be used as a class characteristic
        


---
class: inverse
# Questions?

---
class: primary
## References
&lt;br/&gt;
Deep Learning with R (2018). _Deep Learning with R_. 00007.
Manning Publications Company.

Benedict, I, E. Corke, R. Morgan-Smith, et al. (2014).
"Geographical variation of shoeprint comparison class
correspondences". In: _Science and Justice_ 54.5. 00001, pp.
335-337.

Gross, S, D. Jeppesen and C. Neumann (2013). "The variability and
significance of class characteristics in footwear impressions".
In: _Journal of Forensic Identification_ 63.3. 00001, p. 332.

Hamm, E. D. (1989). "The individuality of class characteristics in
Converse All-Star footwear". In: _Journal of Forensic
Identification_ 39.5. 00008, pp. 277-292.

Shridhar, K, F. Laumann, A. L. Maurin, et al. (2018). "Bayesian
Convolutional Neural Networks with Variational Inference". In:
_arXiv:1806.05978 [cs, stat]_. 00000 arXiv: 1806.05978. URL:
[http://arxiv.org/abs/1806.05978](http://arxiv.org/abs/1806.05978)
(visited on Jan. 17, 2019).
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
