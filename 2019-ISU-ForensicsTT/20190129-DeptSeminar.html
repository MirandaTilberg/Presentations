<!DOCTYPE html>
<html>
  <head>
    <title>CoNNOR: Convolutional Neural Network for Outsole Recognition</title>
    <meta charset="utf-8">
    <meta name="author" content="Susan VanderPlas" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/csafe.css" type="text/css" />
    <link rel="stylesheet" href="css/csafe-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# CoNNOR: Convolutional Neural Network for Outsole Recognition
### Susan VanderPlas
### Jan 29, 2019

---

class: primary











class:primary
## Outline

- Forensics Context

- Image Analysis

- Convolutional Neural Networks

- CoNNOR

- Future Work

---
class:inv-center
# What is the probability the suspect made this shoeprint from the crime scene?

---
class:primary
### &lt;br/&gt;What is P(someone else made this print)?

1. Define the comparison population

2. Sample from the comparison population

3. Identify similar shoes from the comparison population

4. Calculate probability: `$$\frac{\# \text{ similar shoes}}{\text{Size of comparison population}}$$`

???

The easiest way to determine the probability that the suspect made the print is to define the full space of people who could have made the print. Unfortunately, the simple probability solution doesn't necessarily translate into a simple solution for the real world. 

Comparison population, for instance, could be anything from "People in the seminar room right now" to "People who were in Ames, Iowa within the last week". It would also potentially involve defining other characteristics - people out at 8pm, people who attended the Iowa State football game on a certain date, etc. All of these characteristics potentially affect the types of shoes that people might be wearing.

Once the comparison population is defined, then we have to determine what types of shoes they generally wear, and whether any of those shoes are consistent with the crime scene print. As the print at the crime scene might be degraded, this could be a specific make/model/size of shoe (if the print is really good) or it could be one of several models of shoes (if it can be narrowed down that far). 

When we have data on the comparison population's footwear preferences, and have identified shoes that are both worn by the comparison population and consistent with the crime scene print, we can calculate the probability that a random person in the local population made the print.

Unfortunately...

---
class:primary,center,middle
## Comparison Population

&gt; .large[Quantifying the frequency of shoes in a local population is an unsolveable problem]&lt;br/&gt; - Leslie Hammer, March 2018

???

It's generally not that easy. I talked with Leslie Hammer, who is a well-known footwear examiner in the forensics community, shortly after I first started working at CSAFE, and was somewhat shocked to hear her say that the community considers the problem of local population characterization impossible. It's obviously a coincidence, then, that I started laying the groundwork for this project fairly quickly afterwards. /s

The community has legitimate reasons for thinking this is a difficult problem to solve, though.

---
class:primary
## Comparison Population

- No 100% complete database of all shoes 
    - manufacturer, model, size, tread style, manufacturing molds
    
- Shoe purchase data vs. frequency of wear

- Local populations may differ wildly .small[[Ben+14]]

&lt;br/&gt;&lt;br/&gt;
.center[&lt;img src = "problem-definition/snow-boots.jpg" width = "50%" style = "vertical-align:middle;float:middle"/&gt;]
&lt;!-- https://pixnio.com/free-images/2017/05/03/2017-05-03-07-35-18-900x456.jpg --&gt;

???

There are too many manufacturers to keep track of, new models are released all the time, custom shoe markets, and knockoff shoes... hard to get a database with everything. There are databases of tire treads, for instance, but shoes are much more difficult to track -- they don't have to be certified before they're put on the market. 

In addition, many of us have shoes that we've purchased but almost never wear... so using purchase data (even if that had the geographic resolution needed) is not quite accurate either.

Finally, geographic resolution is nearly impossible. If this pair of boots was used to commit a crime in Florida during the summer, in addition to questioning the suspect's sanity, it would be relatively obvious that the shoes are not commonly worn in the local population at the time the crime was committed, and their presence in a suspect's closet would speak for itself to a degree. If the crime was committed in Iowa around this time of year, it would be much less damning to have a pair of similar boots in your closet - they're commonly worn and appropriate for the season.

So with that said, how can we solve the problem of getting data from the local population?

---
class:primary
## Comparison Population&lt;br&gt;Shoes

How to collect data from the local population? 

1. Build a low profile scanner that can be placed in a high traffic area

2. Scan shoes of those walking past

3. Create a local-area database of relevant scans

--

### .center[This is an engineering problem]

???

We could build a gadget that would fit into a pedestrian pathway and would scan the shoes of passers-by. After some data collection in one or more very local areas, we might be able to generalize our spatial sample to the population of interest.

Unfortunately, I didn't take a whole lot of classes in gadget design while I was doing my PhD. This is a problem for engineers, and a couple of very good ones have assured me that this is doable if we can get the budget (hopefully resubmitting that proposal in April). Statisticians have the luxury of making assumptions about the real world, so ...

---
class:primary
## Comparison Population&lt;br&gt;Shoes

Assume a machine exists that can scan shoe outsoles of pedestrians

--

1. Identify relevant features within the scans

--

2. Assess the frequency of similar shoes in the sampled data

--

### .center[These are statistics and machine learning problems]

???

Let's assume that this machine exists and that it's producing image-quality data. If that's the case, we then would need to identify relevant features within the scans and assess how frequently those features occur in the sampled data. If we can get enough features from the scans, we should be able to fairly accurately calculate the probability of such a shoe occurring in the local population. 

The next question is then... What's considered a relevant feature?
---
class:primary
## Relevant Features

Class Characteristics
- Make, Model, Tread pattern, Size, Type of shoe

- Cannot be used to identify an individual match

- Used for exclusion

???

Typically, class characteristics refer to broad descriptors - make, model, tread pattern (not all shoes of the same model have the same tread, and not all shoes wiht the same tread have the same model), size, and shoe type (dress, heel, athletic, etc.)

More broadly, class characteristics are things shared by shoes which are not individualizing characteristics - e.g. characteristics that arise due to the design or manufacture of the shoe. Class characteristics contrast with randomly acquired characteristics, which occur due to random damage either during the manufacturing process or as the shoe is worn.

---
class:primary
## Relevant Features

Use features other than make/model to characterize shoes

- Knockoffs often have very similar tread patterns
- Similar styles have similar tread patterns across brands
- Unknown shoes can still be classified and assessed

&lt;img src = "problem-definition/star_bar_drmartin.png" width = "16%" style = "vertical-align:middle;padding-left:60px;padding-right:30px"/&gt;
&lt;img src = "problem-definition/star_bar_vibram.png" width = "15%" style = "vertical-align:middle;padding-left:30px;padding-right:30px"/&gt;
&lt;img src = "problem-definition/star_bar_timberland.png" width = "16%" style = "vertical-align:middle;padding-left:30px"/&gt;

???

Given that there is no database of shoe models, we'll be better off defining our class-characteristic based features slightly differently. Tread patterns are a component of class characteristics, and we can describe tread patterns using a set of descriptors. The advantage to this approach is that we can consider a set of shoes with similar tread pattern as a "type" and treat them accordingly. This allows us to get a probability without basing it on a database of makes and models, and without requiring that we sample every shoe that might exist in the local population. It's an abstraction to a level that we can reasonably handle with sampling.

After some deliberation and attempts to manually identify features, we settled on 9 geometric features. 

---
class:primary
## Relevant Features

| Bowtie | Chevron | Circle |
| ------ | ------- | ------ |
| ![Bowtie examples](class_examples/bowtie_examples.png) | ![Chevron examples](class_examples/chevron_examples.png) | ![Circle examples](class_examples/circle_examples.png) |

| Line | Polygon | Quadrilateral |
| ---- | ------- | ---- |
| ![Line examples](class_examples/line_examples.png) | ![Polygon examples](class_examples/polygon_examples.png) | ![Quad examples](class_examples/quad_examples.png) |

| Star | Text | Triangle |
| ---- | ---- | -------- |
| ![Star examples](class_examples/star_examples.png) | ![text examples](class_examples/text_examples.png) | ![Triangle examples](class_examples/triangle_examples.png) |

Used to separate shoes by make/model in (small) local samples [GJN13]

???

These nine features are slightly different from those used in gross et al 2013, but many of them overlap. We define bowties, for example, as any roughly quadrilateral shaped entity that has two opposite concave faces; a butterfly also (roughy) meets this definition, so it is included. Lines are defined such that they must be repeated and parallel, and should not be too curved (though this is of course a bit fuzzy). Chevrons are zigzags, but they are allowed to be curved. Stars are features that alternate as concave and convex; so even three-pointed things are stars so long as they're concave in between the points. Polygon is a catch-all class that includes pentagons, hexagons, and octagons (as observed thus far - I haven't noticed any septagons), and was added because there were insufficient pentagons and hexagons individually relative to the other classes.

---
class: inv-center
# &lt;br/&gt;Image Analysis and Feature Detection

???

Now that we've decided which features to use, we have to figure out how to identify them using image analysis. Images are, after all, matrices of data; in color, they are N x M x 3 matrices, where N x M is the image dimension in pixels. 

---
class: primary
## Image Analysis

### Goal: Identify geometric tread features in images of shoe outsoles

- Robust to different lighting conditions, rotation, image quality

- Fast processing of new images

- Identify features that are explainable to practitioners

???

The detection method should be able to handle different lighting conditions, rotation and image quality - if we plan to use this on the data from the scanner that will eventually exist, we need to make sure the method will handle degraded image quality. Even working with images of shoe soles found online, there's a huge variation in image quality and lighting.

In addition, we need for new images to be processed quickly. It's tolerable if the algorithm takes a while to train, but the production model needs to be able to process new data efficiently. 

Finally, we need to be able to explain what this algorithm is doing to practitioners, which means that the features that are identified should be explainable and fairly consistent with how humans would label things.

I started this project with the intention to use various computer vision algorithms to detect very basic image features, then potentially try to reconstruct those into higher-level geometric features.

---
class:primary
## Feature Detection

#### Classic computer vision feature detection methods: 

- Edge, Corner, Blob, Ridge detection

- Template matching: Hough transforms 
    - line, circle, ellipse detection
    - provide location and orientation

--

.pull-left[
Pros
- No training data necessary
- Relatively simple algorithm
]

--

.pull-right[
Cons
- Not robust (fragile tuning parameters)
- Computationally intensive
- Features lack face validity
]

???

There are various detectors used for edges, corners, blobs, and ridges; these are commonly applied to biological images, telescope images, and more. When applied to relatively noisy shoe data, though, they don't always work as expected. The advantage is that there isn't any "training" data necessary in order to use the algorithms at all; the algorithms themselves are also relatively simple. Unfortunately, the tuning parameters that do exist break when lighting changes occur. The algorithms also tend to take a while to execute, and the features they detect don't quite match up with human labeling - corner detectors, for instance, pick out 3x3 pixel level corners in some cases; scaling that up to a macro level is difficult, particularly when corners may be rounded slightly. There's also less guarantee that the same features will be identified when lighting changes slightly, even for the exact same shoe.

Adding the complications with any single low-level algorithm to the fun of reassembling those features into macro-level options, and I decided to consider other alternatives. Google, Facebook, and Microsoft have all been having a lot of fun with neural networks as feature detectors for images, so ... why not?

---
class:primary
## Feature Detection

#### Convolutional neural networks: 
- Structure designed to mimic perceptual pathways in the human visual system
- Ubiquitous in modern image recognition tasks

--

.pull-left[

Pros
- Pre-trained networks available for tuning    
.small[AlexNet, VGG16, ResNet, Inception]
- Features are interpretable
- Very fast (after training)

]

--

.pull-right[

Cons
- Requires labeled training data (lots!)
- Computationally intensive to train
- Opaque - parameters are not interpretable

]

???

There are definite advantages to neural networks - they actually work, have been proven to solve much harder problems than this, and provide human-interpretable features because they're built to mimic how the visual cortex works. There are pre-trained networks available, which dramatically reduces the amount of training necessary to get a functional production model. In addition, these were designed to work at google scale, so they can more than meet our computational needs. 

There are disadvantages too - these models require formal training data, which means that someone has to sit stare at the bottom of thousands of shoes, labeling tread features. They're computationally intensive to train, though not all that bad - between 15 minutes and 3 hours during our initial tests on other datasets. Then there's the big problem  - they have so many parameters that it's hard to interpret them, and they're also not designed for actual inference - getting error estimates out of them is difficult.

But, for the moment, "It works" is going to be the more important part than the nicities of simple models with built-in error estimation.

---
class:inv-center
# &lt;br&gt;Convolutional Neural Networks

???

I'm going to really quickly walk through the specific neural network we used for this task and how it (and similar networks) are fit, and why using a pre-trained network is so awesome. Then we'll walk through the results, some model diagnostics, and I'll talk a bit about some of the more interesting issues we've encountered

---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

.small[VGG16 was trained on an ImageNet [KSH12] dataset of 150,000 images with 1000 output classes [SZ14]]

???

VGG16 is a neural network that is widely used for image recognition tasks. It's a deep convolutional network, which is a reference to the number of layers; you can see that there are several convolutional layers (2-3) for each pooling layer; these stacks of convolutional and pooling layers are then capped off with several dense layers that integrate various features together into a unified whole. The fully connected (aka dense) layers and the softmax layer form the "model head", which transforms spatial information into (usually) probabilistic class predictions.

The convolutional layers are often treated as magic, so I'm going to do my best to ruin that illusion for you.

---
class: primary
## Convolutional Layers

For layer `\(\ell\)` and cell `\(ij\)`, with weights `\(\mathbf{w}\)` 

`$$x^\ell_{ij} = \sum_{a=0}^{m-1}\sum_{a=0}^{m-1} w_{ab} y_{(i+a)(j+b)}^{\ell-1}$$`

The  nonlinear activation function `\(\sigma(\cdot)\)` is then applied: `\(y_{ij}^\ell = \sigma(x_{ij}^\ell)\)` 

Typically, the nonlinear activation function is the Rectified Linear Unit (ReLU), `\(y_{ijk} = \max\{0, x_{ijk}\}\)`

--

This is "Forward Propagation"

???

Simply, a convolutional layer is a combination of a spatial filter of a certain size applied to every possible spatial area on a grid; these filters are composed of weights, and the sum over these weights produces the cell-wise values. Then a nonlinear transformation is applied to the sum. In pictures...

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

Suppose we have an input matrix and a spatial filter. We'd start by applying that filter to each possible 3x3 portion of the input, which produces a 3x3 output matrix. 

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter1.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

So first, we'd add up the input values, multiplied by the filter values - this corresponds to `\(x_{ij}\)` in the numeric formulation shown previously.

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter2.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

--

.move-margin[&lt;br/&gt;&lt;br/&gt;
The activation function is applied to the feature map values
]

???

Then, we'd move on to the next cell and fill in that value; repeating this process for the rest of the valid cells in the input matrix. VGG16 uses a 3x3 receptive field (filter size).

The nonlinear activation function is applied to the feature map values. 

---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

???

It's possible to reduce the lost dimensionality by padding the image; which is what VGG16 does. This way, image size changes only with max-pooling layers, and the image is halved in width and length with each max pooling layers. What do max pooling layers do?

---
class: primary
## Max Pooling Layers

![Pooling Layers](vgg16-structure/maxpooling.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

Simply, they look at a 2x2 area and take the maximum pixel value. Thus, the output region is exactly half of the input region. 

---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

---
class: primary
## Fully Connected Layers
&lt;img src="20190129-DeptSeminar_files/figure-html/unnamed-chunk-5-1.png" width="75%" style="display: block; margin: auto;" /&gt;

???

Fully connected layers are also known as dense layers; they take the spatial information and look for patterns. Numerically, they are used to reduce the dimensionality, and often have "dropout", which reduces the number of parameters by pruning weak connections.

---
class: primary
## Dropout Layers

&lt;!-- ![Dropout layers](vgg16-structure/dropout.png) --&gt;

&lt;!-- .footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2] --&gt;

&lt;img src="20190129-DeptSeminar_files/figure-html/unnamed-chunk-6-1.png" width="75%" style="display: block; margin: auto;" /&gt;

???

Here's a representation of a layer that's fully connected, and the same layer that has a 50% dropout rate.


---
class: primary
## Fitting Mechanism

- Forward Propagation: Input -&gt; Filters -&gt; Pooling -&gt; Result

- Backward Propagation: Errors -&gt; Pooling -&gt; Filters to modify weights
    - Loss function `\(L\)` describing the prediction errors
    - Compute `\(\displaystyle\left(\frac{\partial L}{\partial y_{ij}^\ell}\right)\)` for each cell in the previous layer
    

`$$\left(\frac{\partial L}{\partial w_{ab}}\right) = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} \frac{\partial x^\ell_{ij}}{\partial w_{ab}} = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} y^{\ell-1}_{(i+a)(j+b)}$$`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]

???

We've talked about how weights are used to move from image to predictions; of course, just as with any model fitting process, there's a feedback mechanism where the weights are updated to minimize a loss function. In NNs, this is called backward propagation.

---
class: primary
## Backward Propagation

`\(\displaystyle \frac{\partial L}{\partial x^{\ell}_{ij}}\)` is the gradient component:

`$$\frac{\partial L}{\partial x^{\ell}_{ij}} = \frac{\partial L}{\partial y_{ij}^\ell}\frac{\partial y_{ij}^\ell}{\partial x_{ij}^\ell} = \frac{\partial L}{\partial y_{ij}^\ell} \frac{\partial }{\partial x_{ij}^\ell}\left(\sigma(x_{ij}^\ell)\right) = 
 \frac{\partial L}{\partial y_{ij}^\ell} \sigma'(x_{ij}^\ell)$$`


`\(\displaystyle\frac{\partial L}{\partial y_{ij}^\ell}\)` is error at the current layer

The gradiant can be computed with the derivative of the activation function `\(\sigma(x)\)`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]

???

Very simply, using chain rule, the gradient used to update the weights can be computed using the derivative of the activation function. 

---
class: primary
## Backward Propagation

&lt;br/&gt;
To propagate errors to the previous layer, 

`$$\frac{\partial L}{\partial y_{ij}^{\ell -1}} = \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell}\frac{\partial x_{(i-a)(j-b)}^\ell}{\partial y_{ij}^{\ell-1}}
= \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell} \omega_{ab}$$`

&lt;br/&gt;
.center[&lt;img src="vgg16-structure/vgg16-shoe.png" width = "50%"/&gt;]

.center[&lt;b&gt;13 convolutional layers = a lot of backpropagation&lt;/b&gt;]

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]

???

Then, with a bit more algebra, we can move the errors from the current layer to a previous layer, and use that to update weights there.

This process is repeated a lot.

---
class: primary
## VGG16

&lt;br/&gt;
- Convolutional base of VGG16 for 256 x 256 x 3 images has __14,714,688__ parameters

- Head has __155,090,944__ parameters and outputs __1000__ different class label predictions. 

- Originally fit using __150,000__ images

--

- We have __9__ class labels and __&lt;25,000__ labeled images

- Solution: Use the pre-trained VGG16 base (all of the convolutional layers) and train a new head
    - Reduces the parameter space to __8,391,177__ parameters

???

With all of the backpropagation, you can about imagine how many parameters VGG16 has. If you can't, for an input image size of 256x256 with full color (3 channels), there are more than 14.5 million parameters just in the convolutional base. In the head, with multiple fully connected layers, there are just over 155 million parameters; the default is to predict on 1000 different class labels. To find initial values for these parameters, they used 150,000 different images.

We only have 9 classes, each corresponding to a geometric shape; we have less than 25K images. We can't train the full thing; instead, we'll use the base of VGG16 and just optimize a new head consisting of a fully connected layer and a softmax activation layer. A very basic model head. 

In really simple terms, we're hacking off the higher brain functions and limiting this model to pre-k.

---
class: inv-center
# Fitting CoNNOR: Convolutional Neural Network for Outsole Recognition

???

Now that you have an idea of what's happening inside the computer, we're going to focus on the more human side of the equation.

---
class:primary
## Acquire Data
&lt;img alt="Zappos Screenshot showing sole images" src="labelme-imgs/zappos.png" width="90%" style="margin: 0 5%"/&gt;

.move-margin[&lt;br/&gt;&lt;br/&gt;49745 shoe outsole images scraped since April 2018]

???

To start with, where'd the data come from? I built a scraper to pull images from Zappos; the images are relatively high resolution (usually 1920x1400). It's been running since late April, and we have about 50K images at this point.

The next step was to get humans to label things in the same way we want the model to label things.

---
class:primary
## Label Data
![LabelMe](labelme-imgs/LabelMe1.png)

- 24,122 regions labeled with one or more geometric objects

- 31,806 labels

- [LabelMe Annotation Tool](https://github.com/CSAILVision/LabelMeAnnotationTool) used as a web interface - creates XML files with labels and coordinates. [Rus+08]


--
.small[.move-margin[
&lt;br/&gt;&lt;br/&gt;Labeling courtesy of 
- Jenny Kim
- Ben Wonderlin
- Mya Fisher
- Holden Jud
- Miranda Tilton
- Charlotte Roiger
- Joe Zemmels
- and others
]]

???

The real heroes here are the high schoolers, undergraduates, and grad students who spent time labeling images to provide training data for the model. 

One issue we ran into is that our criteria changed as we found new edge cases that forced us to make a decision on whether something was part of a specific category. In addition, initially we were using the Matlab toolkit that came with the LabelMe program to process and extract the images from the annotation files. Over the summer, I rewrote that code in R to customize it to our needs. This rewrite changed some of the previous guidelines - instead of labeling multiple features that overlap separately, we now label all of the features within a region with the same polygon. This reflects a change in how modeling occurs as well as a change in the processing code. Unfortunately, we're still cleaning up the annotations that were completed before that point. 

---
class:primary
## Label Data
&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/unnamed-chunk-7-1.png" alt="Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon." width="90%" /&gt;
&lt;p class="caption"&gt;Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon.&lt;/p&gt;
&lt;/div&gt;

???

This graph reflects the current state of class labels. There are far more quadrilaterals, lines, and text than any other category; quads in particular are more likely to appear alone than in multiple groups. Stars, polygons, and bowties are much less likely to occur; this large discrepancy in class frequency does make modeling more interesting. 

---
class:primary
## Model Specification
Multiple classes, multiple labels: "One-hot" encoding

Statistically: 

- Model output: `\((P_1, ..., P_9) \sim Bernoulli(\mathbf{\pi})\)`
    - Each geometric feature assigned a probability
    - An image can be labeled with multiple features

- Output probabilities are not independent
    - Dependencies due to CNN structure
    - Dependencies due to input data
    - Dependencies due to geometry -     
    Polygons vs. Quadrilaterals vs. Lines
    
- Covariance structure is ?

--

.move-margin[&lt;br/&gt;&lt;br/&gt;There are downsides to black-box models...]

???

We've set the model up so that it predicts the presence or absence of each of 9 classes. In machine learning, this is "one-hot" encoding, but you may be more familiar with it as coding categories using indicator variables. 

What's important to note is that because any given image can have between 0 and 9 class labels, each of the 9 probabilities produced by the model are between 0 and 1, but there are no other constraints - they do not have to sum to 1. 

We can think of this as a multivariate bernoulli situation, where each label is assigned with probability `\(\pi_i\)`. The `\(pi\)`s are not independent - there is obviously a dependence both due to feature similarity (e.g. polygons and quads have similar features) and input data - easily identifiable classes are more likely to have been labeled. There's also a dependency due to the CNN structure, but I'd be hard pressed to describe it beyond saying that it exists.

So any attempts to think too hard about the covariance structure are (at this point) doomed. There are downsides to using a black-box model; one of those is that statistical formalities are not necessarily observed. Of course, the upside is that the model actually works.


---
class:primary
## Model Training

- 256 x 256 pixel images

- Training data (60%):
    - 1x Augmented images (rotation, skew, zoom, crop) to prevent overfitting
    - Class weights used to counteract uneven class sizes
    
- Validation and test data (20% each)

- Miranda Tilton fit the model using the `keras` package in R, which provides a high-level API for the `tensorflow` library [CA18; Aba+15]

???

We scaled all of the labeled images to 256 x 256; aspect ratio was not preserved, though some steps have been taken to ensure that the labeled regions are at least square-ish where possible to prevent extreme distortion. 

60% of the labeled images were used as training data; these images were augmented once by zooming, skewing, cropping, and rotating the images. This step is recommended to prevent over-fitting, and is particularly important given that many features (e.g. bowties in Birkenstocks) are similar across shoes, so an image that is essentially the same may appear in the training, validation, and test sets.

Validation data, which is used within each fitting iteration to calculate the loss function, accounted for 20% of the images, and test data, which is used to evaluate the model at the end of the fitting process, accounted for the remaining 20%. 

We used the keras package in R to fit the model using the tensorflow toolkit. Tensorflow is an extremely efficient implementation that can use either the CPU or GPU to fit the neural network. It was originally developed by Google's Machine Intelligence team. Keras makes it easy to use VGG16, remove the model head, freeze the weights on the base, and add a new head, using only a few lines of code. 

---
class:primary
## Model Training

&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/training-accuracy-1.png" alt="Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy." width="99%" /&gt;
&lt;p class="caption"&gt;Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.&lt;/p&gt;
&lt;/div&gt;

???

This chart shows model performance relative to the loss and accuracy rate during each epoch (backpropagation occurs after each epoch of fitting). 

Validation loss levels off after 15 epochs, but hasn't yet begun to increase. Training loss is still decreasing as well. One concern with retraining the head of a CNN is that with relatively little data (e.g. 20 thousand data points instead of 150K) it is easy to over-fit models; what we see is that this hasn't yet happened for this model. Overfitting would be evident if the loss in the training set had beun to increase. 

---
class:primary
## Evaluating the Model
&lt;img src="20190129-DeptSeminar_files/figure-html/unnamed-chunk-8-1.png" width="100%" /&gt;

???

We can compute an aggregate ROC curve that treats all classes the same. Under this, we see that performance is generally fairly good, though there is obviously room for improvement. The more interesting evaluation is to look at prediction accuracy for each label...

---
class:primary
## Evaluating the Model
&lt;!-- Add in model overall AUC --&gt;
&lt;!-- Describe the multi-class version as splitting out model performance by class --&gt; 

&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/unnamed-chunk-9-1.png" alt="Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class." width="99%" /&gt;
&lt;p class="caption"&gt;Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class.&lt;/p&gt;
&lt;/div&gt;

???

These plots show ROC curves for each class, computed separately. Because this problem is a multivariate binary classification problem (in statistical parlance), we can adapt most of the diagnostic plots we're used to with a few modifications. In this case, each ROC curve was generated without consideration of other plot features.

Equal error rates are marked with a dot, and show the point at which it is equally likely for the model to miss a classification or wrongly classify an image. These EERs are used as an optimized cutoff value for diagnostics which require a hard threshold, like a confusion matrix.

---
class:primary
## Evaluating the Model
&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/ConfMatrix-1.png" alt="Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes." width="80%" /&gt;
&lt;p class="caption"&gt;Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes.&lt;/p&gt;
&lt;/div&gt;

.move-margin[

&lt;br/&gt;
For multi-label images, only incorrect predictions contribute to off-diagonal probabilities

`\(P &gt; EER_i\)` used as the cutoff
&lt;br/&gt;

]

???

This confusion matrix shows, for each label, the probability that the image is classified as that label as well as other possible labels. One modification we made to the standard confusion matrix was to exclude any additional "correct" labels from these calculations: If an image was labeled with a circle and a line, but the model assigned circle and triangle as labels, then in the circle column that image would register as a true positive for circle and a false positive for triangle; line would be excluded from calculations in that column. Similarly, in the line column, triangle would be marked as a false positive, circle would be excluded, and line would be marked as a false negative. 

An important point to make at this juncture is that while we're operating as if our labeled data were "ground truth", that isn't an accurate assumption. People make mistakes, labeling is monotonous, and the criteria for certain classes have changed over time. In some cases, the model is correct, and the labels are wrong. We're working on correcting the labeling, but even in a situation where the labeling is done in accordance with the guidelines, some of the criteria can get fuzzy in practice. 

---
class:primary
## Definitions matter
![Classes get confusing](labelme-imgs/dc_circle_quad_confusion.png)

--

![Not everything is labeled correctly](labelme-imgs/circle_pred_correct.png)

???

We created a shiny application to see the images and the model's predictions. Blue means that the image had that label, grey means it does not. 

So in the first image, the design is labeled as a quadrilateral (rounded quadrilateral) rather than a circle. We're still debating how to handle these... they're very ambiguous figures with features of both circles and quadrilaterals. The model is not wrong, but neither is the labeling... or maybe both of them are wrong?

In the second image, both text and quad are labels, but the model also identifies a circle in the text with probability 0.31; that is, the text contains an O. 

We use the shiny application to screen for these problems so that we can correct the labeled training data where it does not meet the current guidelines for labeling. When we find an issue that's widespread (e.g. the Adidas logo is often not labeled as containing quads) we examine the shoes matching that description (so some screening occurs outside of what is predicted by the model). 

We're trying to ensure that the data used to train the model is of very high quality, while not spending millions of dollars to hire workers online to label things. Because we determined the guidelines for labeling the data, labeled the data (or oversaw the labeling), and trained the model ourselves, we have the advantage of knowing the flaws at every point in the process; that means we have the responsibility to fix those flaws where possible. 

We're not doing inference on the model results at this point (nor planning to use the data we're training the model with during the operational stage) so the data -&gt; model -&gt; fix data loop is less of a validity concern. 

When the model is sufficiently well-calibrated, we can then work with engineers to build the device, collect some initial data, and tweak the model weights with new data that better represents what we'll actually see from the collection equipment. By that point, hopefully we'll also have narrowed down the geometric classification scheme so that categories that are now somewhat fuzzy are more clearly operationalized.


---
class:primary
## Debugging the model

- Active area of research: How to debug, interpret, and understand what a CNN is actually doing    
.small[https://distill.pub/2018/building-blocks/]

- Which regions in the image are relevant to the class?

- Which filters are most important for detection of each class?

???

During this process, it's also helpful to see what the model is using to determine which features are present or absent. Black box models are notoriously difficult to debug, but this is a very active area of current research. 

In particular, we'd like to know which regions in the image are relevant to the class, and which filters in the model are important for detecting each class. We've made some progress on the first question, and are actively looking into the second. 





---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - DC](heatmaps/heatmap-quad-4-dc-pure-se-navy_product_7270757_color_9.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

Class activation maps use the gradient with respect to each class label, back propagated to the last convolutional layer to maintain spatial information. Here, we can see that the D shape discussed previously is activating both circle and quad; the round part is activating the circle class and the straight part is activating the quad class; there's also some slight activation of the polygon class, which isn't surprising given that polygons and quads share some features. 

---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - adidas](heatmaps/heatmap-text-1-adidas-kids-adilette-clf-adj-toddler-little-kid-big-kid-black-white_product_8987203_color_151.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

This is a piece of the Adidas text that is part of the design of many of their shoe treads. The model is obviously cuing into the text, but interestingly, it also detects circles - the text is so round that the inside of any A or D character is a perfect circle. While we wouldn't necessarily go out of our way to label that, because text dominates the human brain, we've created a model that doesn't have that dominance; the circles are just as real as the text to this model, and should be labeled as such.


---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - seychelles](heatmaps/heatmap-text-2-seychelles-slow-down-blush-metallic_product_9017725_color_34700.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

In a similar instance, the model cues in on circles in script S characters as well. These are objectively not circles, though, as they aren't closed, so we do not update the labeling for these images. It's a delicate balance between "the model isn't wrong" and tweaking the labels to match the model output exactly - we're working to find that balance. 

---
class:inv-center
# &lt;br/&gt;&lt;br/&gt;What's Next?

---
class:primary
## Debugging the model
- Port the `keras-vis` Python library to R (In progress!)
    - Activation Maximization maps: generate a new image to maximize filter output activations
        - Show what specific filters are doing and "seeing"
        
    - What parts of an image are most important in activating a specific class?
        - Saliency Maps: plot gradients w.r.t. output
        - Class Activation Maps: plot gradients w.r.t. last conv. layer (maintains spatial information)
        
&lt;!-- Insert Jason's stuff here --&gt;        

???

I'm currently supervising an undergrad CS/stats major who's working to create an R package that interfaces with the keras-vis python library. He's had some success in the past couple of weeks, but he's learning R at the same time he's doing this. When he's got things working, we should be able to get much better insight into what specific model filters are doing and seeing. We should also be able to identify which filters are most useful, which may allow us to build a scaled-down model base eventually. 

---
class: primary
## Whole-Shoe Predictions
- Currently, predictions are for 256 x 256 chunks

- Integrate multiple chunks to provide geometric features with probabilities and relative coordinates

- Add spatial information to further discriminate between shoe models

- Integrate the current model with a database of make/model/tread information to use for automatic identification of shoe soles.

???

I'm also hoping to tackle the whole-shoe integration problem. If a shoe is divided up into 256x256 chunks, it's going to be interesting to try to integrate the chunks (and the predictions) back into a single unified whole. That would provide additional spatial information and would also potentially be useful to speed up image searches in a database of shoe make/model information. Sure, it'll never be 100% complete, but most shoes aren't custom. 

---
class: primary
## Collect local population data

- Build the machine (or rather, have engineers build it...)
- Collect data from (very) local areas
    - date/time stamp
    - side-view pictures of the shoes (where possible)
    - scans of the bottom of the shoe
- Label geometric features on the collected scans
- Run a 2nd stage of model weight optimization with scanned data
- Use this optimized model to analyze newly collected data

???

I'm also hoping that with this year's NIJ grants we may get funding to actually build the machine. That will allow us to start testing local data collection, and hopefully to tweak the model weights for the specific images that the machine eventually produces. 

---
class: primary
## Local population data

- Use spatial locations + label probabilities as a feature vector

- Database of shoes in local population can be used to assess likelihood of a specific feature set occurring at random
    - Helpful to have error estimates for CNN output probabilities 
        - Bayesian CNNs [Shr+18]
    - Useful to know wear pattern frequency
        - As shoes wear, geometric features become less pronounced
        - Wear patterns can also be used as a class characteristic
        
???

Eventually, the goal is to take the whole shoe predictions and label probabilities, use that as a feature vector, and assess the likelihood that the print was made by the suspect compared to the likelihood the print was made by someone in the local population. 

To do this, we may end up having to find some error estimates for CNN output probabilities, and it's possible we'll end up examining wear pattern frequency as well - as tread patterns degrade, wear patterns become more pronounced, which is a nice trade-off. 

---
class: inv-center
# Questions?

---
class: primary
## References
&lt;div class="small"&gt;

&lt;p&gt;[1]&lt;cite&gt;
M. 
Abadi, A. Agarwal, P. Barham, et al.
&lt;em&gt;TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems&lt;/em&gt;.
Software available from tensorflow.org.
2015.
URL: &lt;a href="http://tensorflow.org/"&gt;http://tensorflow.org/&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;cite&gt;
I. Benedict, E. Corke, R. Morgan-Smith, et al.
&amp;ldquo;Geographical variation of shoeprint comparison class correspondences&amp;rdquo;.
In: &lt;em&gt;Science and Justice&lt;/em&gt; 54.5 (2014), pp. 335&amp;ndash;337.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[3]&lt;cite&gt;
F. Chollet and J. J. Allaire.
&lt;em&gt;Deep Learning with R&lt;/em&gt;.
Manning Publications Company, 2018.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[4]&lt;cite&gt;
S. Gross, D. Jeppesen and C. Neumann.
&amp;ldquo;The variability and significance of class characteristics in footwear impressions&amp;rdquo;.
In: &lt;em&gt;Journal of Forensic Identification&lt;/em&gt; 63.3 (2013), p. 332.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[5]&lt;cite&gt;
A. Krizhevsky, I. Sutskever and G. E. Hinton.
&amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo;.
In: 
&lt;em&gt;Advances in Neural Information Processing Systems 25&lt;/em&gt;.
Ed. by F. Pereira, C. J. C. Burges, L. Bottou and K. Q. Weinberger.
Curran Associates, Inc., 2012, pp. 1097&amp;ndash;1105.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[6]&lt;cite&gt;
B. C. Russell, A. Torralba, K. P. Murphy, et al.
&amp;ldquo;LabelMe: A Database and Web-Based Tool for Image Annotation&amp;rdquo;.
En.
In: &lt;em&gt;International Journal of Computer Vision&lt;/em&gt; 77.1-3 (May. 2008). 02464, pp. 157&amp;ndash;173.
ISSN: 0920-5691, 1573-1405.
DOI: &lt;a href="https://doi.org/10.1007/s11263-007-0090-8"&gt;10.1007/s11263-007-0090-8&lt;/a&gt;.
URL: &lt;a href="http://link.springer.com/10.1007/s11263-007-0090-8"&gt;http://link.springer.com/10.1007/s11263-007-0090-8&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[7]&lt;cite&gt;
K. Shridhar, F. Laumann, A. L. Maurin, et al.
&amp;ldquo;Bayesian Convolutional Neural Networks with Variational Inference&amp;rdquo;.
In: &lt;em&gt;arXiv:1806.05978 [cs, stat]&lt;/em&gt; (Jun. 2018).
URL: &lt;a href="http://arxiv.org/abs/1806.05978"&gt;http://arxiv.org/abs/1806.05978&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[8]&lt;cite&gt;
K. Simonyan and A. Zisserman.
&amp;ldquo;Very Deep Convolutional Networks for Large-Scale Image Recognition&amp;rdquo;.
In: &lt;em&gt;arXiv:1409.1556 [cs]&lt;/em&gt; (Sep. 2014).
URL: &lt;a href="http://arxiv.org/abs/1409.1556"&gt;http://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;
&lt;/div&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
