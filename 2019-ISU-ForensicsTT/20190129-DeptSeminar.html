<!DOCTYPE html>
<html>
  <head>
    <title>CoNNOR: Convolutional Neural Network for Outsole Recognition</title>
    <meta charset="utf-8">
    <meta name="author" content="Susan VanderPlas" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/csafe.css" type="text/css" />
    <link rel="stylesheet" href="css/csafe-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# CoNNOR: Convolutional Neural Network for Outsole Recognition
### Susan VanderPlas
### Jan 29, 2019

---

class: primary











## About Me

- PhD from Iowa State in 2015

- Statistician at Nebraska Public Power District (2015 - 2018)

- Research Faculty at CSAFE since March 2018

- Research Areas: Visualization and Statistical Computing, Forensics

???

Thank you for inviting me to interview for this position. I may look familiar to some of you - I graduated from this department in 2015. While I was here as a student I did research into graphics, visualization, and computing; I also collaborated with people in engineering and bioinformatics research. I left Iowa State to work at Nebraska Public Power District, where I helped to build a data science group and teach data analysis and computing skills internally. For the past 11 months, I've been working at CSAFE as research faculty. During that time, I've worked on projects involving human factors, shoes, and bullets. 

---
class:primary

## Graphics Publications

.slightly-small[
&lt;p&gt;[1]&lt;cite&gt;
S. VanderPlas and H. Hofmann.
&amp;ldquo;Signs of the Sine Illusion - Why We Need to Care&amp;rdquo;.
In: &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 24.4 (Oct. 2015), pp. 1170&amp;ndash;1190.
ISSN: 1061-8600.
DOI: &lt;a href="https://doi.org/10.1080/10618600.2014.951547"&gt;10.1080/10618600.2014.951547&lt;/a&gt;.
(Visited on 12/12/2018).&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;cite&gt;
S. VanderPlas and H. Hofmann.
&amp;ldquo;Spatial Reasoning and Data Displays&amp;rdquo;.
In: &lt;em&gt;IEEE Transactions on Visualization &amp;amp; Computer Graphics&lt;/em&gt; 22.1 (Jan. 2016), pp. 459-468.
ISSN: 1077-2626.
DOI: &lt;a href="https://doi.org/10.1109/TVCG.2015.2469125"&gt;10.1109/TVCG.2015.2469125&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[3]&lt;cite&gt;
H. Hofmann and S. VanderPlas.
&amp;ldquo;All of This Has Happened Before. All of This Will Happen Again: Data Science&amp;rdquo;.
In: &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 26.4 (2017), pp. 775&amp;ndash;778.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[4]&lt;cite&gt;
S. VanderPlas and H. Hofmann.
&amp;ldquo;Clusters Beat Trend!? Testing Feature Hierarchy in Statistical Graphics&amp;rdquo;.
In: &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; 26.2 (Apr. 2017), pp. 231&amp;ndash;242.
ISSN: 1061-8600.
DOI: &lt;a href="https://doi.org/10.1080/10618600.2016.1209116"&gt;10.1080/10618600.2016.1209116&lt;/a&gt;.
(Visited on 12/12/2018).&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[5]&lt;cite&gt;
S. VanderPlas, R. Goluch and H. Hofmann.
&amp;ldquo;Framed! Reproducing and Revisiting 150 year old charts&amp;rdquo;.
In: &lt;em&gt;Journal of Computational and Graphical Statistics&lt;/em&gt; (2019).
ISSN: 1537-2715.
DOI: &lt;a href="https://doi.org/10.1080/10618600.2018.1562937"&gt;10.1080/10618600.2018.1562937&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[In Progress]&lt;cite&gt;
S. VanderPlas and M. Tilton.
&amp;ldquo;Truthiness, Maps, and Graphs&amp;rdquo;&lt;/cite&gt;&lt;/p&gt;

]

???

I've published several papers in JCGS and TVCG (Transactions on Visualization and Computer Graphics). I'm currently working on a project involving the effect of charts and graphs on the evaluation of factual statements; a pilot study was completed in early December and the full study will hopefully be completed this semester. 

---
class:primary
## Forensics
.slightly-small[
### Papers

&lt;p&gt;[In Progress]&lt;cite&gt;
S. VanderPlas.
&amp;ldquo;BulletsamplR: Resampling Bullet Cross Sections&amp;rdquo;&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[In Progress]&lt;cite&gt;
S. VanderPlas, T. Klep, M. Nally, C. Cadeval, and H. Hofmann.
&amp;ldquo;Case study validations of automatic bullet matching.&amp;rdquo;&lt;/cite&gt;&lt;/p&gt;

&lt;p style = "font-weight:700"&gt;[In Progress]&lt;cite&gt;
M. Tilton and S. VanderPlas
&amp;ldquo;CoNNOR: A Convolutional Neural Network for Outsole Recognition.&amp;rdquo;&lt;/cite&gt;&lt;/p&gt;

### Grants

&lt;p&gt; &lt;cite&gt;NIJ R&amp;D in Forensic Science. &amp;ldquo;Statistical Infrastructure for the Use of Error Rate Studies in the Interpretation of Forensic Evidence.&amp;rdquo; Collaborator. Funded for FY 2019&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt; [In Progress] &lt;cite&gt;NIJ R&amp;D in Forensic Science. &amp;ldquo;Passive Acquisition of Footwear Class Characteristics in Local Populations&amp;rdquo; PI. Submission in 2019 for FY 2020&lt;/cite&gt;&lt;/p&gt;
]

???

Since I started at CSAFE in March, I've worked on several projects that are nearing completion. I'm close to finishing a paper discussing methods for resampling bullet signatures while preserving continuity and other relevant characteristics; Another paper on validation of the automated bullet matching algorithm is ready for submission to the Journal of Forensic Science. I'm also collaborating with Danica Ommen on a grant to develop effective visualizations for Bayes Factors and ROC curves in forensics. 

Today I'm going to talk about one of the larger projects I've worked on in the past year; 

---
class:primary
## Outline

- Forensics Context

- Image Analysis

- Convolutional Neural Networks

- CoNNOR

- Future Work

---
class:primary
## What is the probability of&lt;br&gt;a coincidental match?

&lt;img src="problem-definition/coincidental-match.png" width = "80%" style="margin-left:10%;margin-right:10%" alt="Crime scene print compared with sample print"/&gt;


---
class:primary
## Coincidental Match

1. Define the comparison population

2. Sample from the comparison population

3. Identify similar shoes from the comparison population

4. Estimate the probability: `$$\frac{\# \text{ similar shoes}}{\text{Size of comparison population sample}}$$`

???

The easiest way to determine the probability that the suspect made the print is to define the full space of people who could have made the print. Unfortunately, the simple probability solution doesn't necessarily translate into a simple solution for the real world. 

Comparison population, for instance, could be anything from "People in the seminar room right now" to "People who were in Ames, Iowa within the last week". It would also potentially involve defining other characteristics - people out at 8pm, people who attended the Iowa State football game on a certain date, etc. All of these characteristics potentially affect the types of shoes that people might be wearing.

Once the comparison population is defined, then we have to determine what types of shoes they generally wear, and whether any of those shoes are consistent with the crime scene print. As the print at the crime scene might be degraded, this could be a specific make/model/size of shoe (if the print is really good) or it could be one of several models of shoes (if it can be narrowed down that far). 

When we have data on the comparison population's footwear preferences, and have identified shoes that are both worn by the comparison population and consistent with the crime scene print, we can calculate the probability that a random person in the local population made the print.

Unfortunately...

---
class:primary,center,middle
## Comparison Population

&gt; .large[Quantifying the frequency of shoes in a local population is an unsolveable problem]&lt;br/&gt; - Leslie Hammer, [Hammer Forensics](https://hammerforensics.com/), March 2018

???

It's generally not that easy. I talked with Leslie Hammer, who is a well-known footwear examiner in the forensics community, shortly after I first started working at CSAFE, and was somewhat shocked to hear her say that the community considers the problem of local population characterization impossible. It's obviously a coincidence, then, that I started laying the groundwork for this project fairly quickly afterwards. /s

The community has legitimate reasons for thinking this is a difficult problem to solve, though.

---
class:primary
## Comparison Population

- No 100% complete database of all shoes 
    - manufacturer, model, size, tread style, manufacturing molds
    
- Shoe purchase data vs. frequency of wear

- Local populations may differ wildly .small[(Benedict, et al., 2014)]

&lt;br/&gt;&lt;br/&gt;
.center[&lt;img src = "problem-definition/snow-boots.jpg" width = "50%" style = "vertical-align:middle;float:middle"/&gt;]
&lt;!-- https://pixnio.com/free-images/2017/05/03/2017-05-03-07-35-18-900x456.jpg --&gt;

???

There are too many manufacturers to keep track of, new models are released all the time, custom shoe markets, and knockoff shoes... hard to get a database with everything. There are databases of tire treads, for instance, but shoes are much more difficult to track -- they don't have to be certified before they're put on the market. 

In addition, many of us have shoes that we've purchased but almost never wear... so using purchase data (even if that had the geographic resolution needed) is not quite accurate either.

Finally, geographic resolution is nearly impossible. If this pair of boots was used to commit a crime in Florida during the summer, in addition to questioning the suspect's sanity, it would be relatively obvious that the shoes are not commonly worn in the local population at the time the crime was committed, and their presence in a suspect's closet would speak for itself to a degree. If the crime was committed in Iowa around this time of year, it would be much less damning to have a pair of similar boots in your closet - they're commonly worn and appropriate for the season.

So with that said, how can we solve the problem of getting data from the local population?

---
class:primary
## Comparison Population&lt;br&gt;Shoes

How to collect data from the local population? 

1. Build a low profile scanner that can be placed in a high traffic area

2. Scan shoes of those walking past

3. Create a local-area database of relevant scans

--

### .center[This is an engineering problem]

???

We could build a gadget that would fit into a pedestrian pathway and would scan the shoes of passers-by. After some data collection in one or more very local areas, we might be able to generalize our spatial sample to the population of interest.

Unfortunately, I didn't take a whole lot of classes in gadget design while I was doing my PhD. This is a problem for engineers, and a couple of very good ones have assured me that this is doable if we can get the budget (hopefully resubmitting that proposal in April). Statisticians have the luxury of making assumptions about the real world, so ...

---
class:primary
## Comparison Population&lt;br&gt;Shoes

Assume a machine exists that can scan shoe outsoles of pedestrians

--

1. Identify relevant features within the scans

--

2. Define similarity for shoe images

--

3. Assess the frequency of similar shoes in the sampled data

--

### .center[These are statistics and machine learning problems]

???

Let's assume that this machine exists and that it's producing image-quality data. If that's the case, we then would need to identify relevant features within the scans and assess how frequently those features occur in the sampled data. If we can get enough features from the scans, we should be able to fairly accurately calculate the probability of such a shoe occurring in the local population. 

The next question is then... What's considered a relevant feature?

---
class:primary
## Relevant Features

Class Characteristics
- Make, Model, Tread pattern, Size, Type of shoe

- Cannot be used to identify an individual match

- Used for exclusion

???

Typically, class characteristics refer to broad descriptors - make, model, tread pattern (not all shoes of the same model have the same tread, and not all shoes wiht the same tread have the same model), size, and shoe type (dress, heel, athletic, etc.)

More broadly, class characteristics are things shared by shoes which are not individualizing characteristics - e.g. characteristics that arise due to the design or manufacture of the shoe. Class characteristics contrast with randomly acquired characteristics, which occur due to random damage either during the manufacturing process or as the shoe is worn.

---
class:primary
## Relevant Features

Use features other than make/model to characterize shoes

- Knockoffs often have very similar tread patterns
- Similar styles have similar tread patterns across brands
- Unknown shoes can still be classified and assessed

| Dr. Martens | Eastland | Timberland |
| --- | --- | --- |
| &lt;img src = "problem-definition/dr-martens-work-2295-rigger-tan-greenland_product_114677_color_201711.jpg" max-width = "40%" height = "250px" style = "padding-left:25%;padding-right:25%"/&gt; | &lt;img src = "problem-definition/eastland-1955-edition-jett-brown_product_8946957_color_6.jpg" max-width = "40%" height = "250px" style = "padding-left:25%;padding-right:25%"/&gt; | &lt;img src = "problem-definition/timberland-6-premium-boot-coal-waterbuck_product_8906913_color_761877.jpg" max-width = "40%" height = "250px" style = "padding-left:25%;padding-right:25%"/&gt; |
| Work 2295 Rigger | 1955 Edition Jett | 6" Premium Boot |

???

Given that there is no database of shoe models, we'll be better off defining our class-characteristic based features slightly differently. Tread patterns are a component of class characteristics, and we can describe tread patterns using a set of descriptors. The advantage to this approach is that we can consider a set of shoes with similar tread pattern as a "type" and treat them accordingly. This allows us to get a probability without basing it on a database of makes and models, and without requiring that we sample every shoe that might exist in the local population. It's an abstraction to a level that we can reasonably handle with sampling.

After some deliberation and attempts to manually identify features, we settled on 9 geometric features. 

---
class:primary
## Relevant Features

| Bowtie | Chevron | Circle |
| ------ | ------- | ------ |
| ![Bowtie examples](class_examples/bowtie_examples.png) | ![Chevron examples](class_examples/chevron_examples.png) | ![Circle examples](class_examples/circle_examples.png) |

| Line | Polygon | Quadrilateral |
| ---- | ------- | ---- |
| ![Line examples](class_examples/line_examples.png) | ![Polygon examples](class_examples/polygon_examples.png) | ![Quad examples](class_examples/quad_examples.png) |

| Star | Text | Triangle |
| ---- | ---- | -------- |
| ![Star examples](class_examples/star_examples.png) | ![text examples](class_examples/text_examples.png) | ![Triangle examples](class_examples/triangle_examples.png) |

Used to separate shoes by make/model in (small) local samples (Gross, et al., 2013)

???

These nine features are slightly different from those used in gross et al 2013, but many of them overlap. We define bowties, for example, as any roughly quadrilateral shaped entity that has two opposite concave faces; a butterfly also (roughy) meets this definition, so it is included. Lines are defined such that they must be repeated and parallel, and should not be too curved (though this is of course a bit fuzzy). Chevrons are zigzags, but they are allowed to be curved. Stars are features that alternate as concave and convex; so even three-pointed things are stars so long as they're concave in between the points. Polygon is a catch-all class that includes pentagons, hexagons, and octagons (as observed thus far - I haven't noticed any septagons), and was added because there were insufficient pentagons and hexagons individually relative to the other classes.

---
class: inv-center
# &lt;br/&gt;Image Analysis and Feature Detection

???

Now that we've decided which features to use, we have to figure out how to identify them using image analysis. Images are, after all, matrices of data; in color, they are N x M x 3 matrices, where N x M is the image dimension in pixels. 

---
class: primary
## Image Analysis

### Goal: Identify geometric tread features in images of shoe outsoles

- Robust to different lighting conditions, rotation, image quality

- Fast processing of new images

- Identify features that are explainable to practitioners

???

The detection method should be able to handle different lighting conditions, rotation and image quality - if we plan to use this on the data from the scanner that will eventually exist, we need to make sure the method will handle degraded image quality. Even working with images of shoe soles found online, there's a huge variation in image quality and lighting.

In addition, we need for new images to be processed quickly. It's tolerable if the algorithm takes a while to train, but the production model needs to be able to process new data efficiently. 

Finally, we need to be able to explain what this algorithm is doing to practitioners, which means that the features that are identified should be explainable and fairly consistent with how humans would label things.

I started this project with the intention to use various computer vision algorithms to detect very basic image features, then potentially try to reconstruct those into higher-level geometric features.

---
class:primary
## Feature Detection

#### Classic computer vision feature detection methods: 

- Edge, Corner, Blob, Ridge detection

- Template matching: Hough transforms
    - line, circle, ellipse detection
    - provide location and orientation

.pull-left[
Pros
- No training data necessary

- Relatively simple algorithm
]

.pull-right[
Cons
- Not robust (fragile tuning parameters)
- Computationally intensive
- Features lack face validity
]

???

There are various detectors used for edges, corners, blobs, and ridges; these are commonly applied to biological images, telescope images, and more. When applied to relatively noisy shoe data, though, they don't always work as expected. The advantage is that there isn't any "training" data necessary in order to use the algorithms at all; the algorithms themselves are also relatively simple. Unfortunately, the tuning parameters that do exist break when lighting changes occur. The algorithms also tend to take a while to execute, and the features they detect don't quite match up with human labeling - corner detectors, for instance, pick out 3x3 pixel level corners in some cases; scaling that up to a macro level is difficult, particularly when corners may be rounded slightly. There's also less guarantee that the same features will be identified when lighting changes slightly, even for the exact same shoe.

Adding the complications with any single low-level algorithm to the fun of reassembling those features into macro-level options, and I decided to consider other alternatives. Google, Facebook, and Microsoft have all been having a lot of fun with neural networks as feature detectors for images, so ... why not?

---
class:primary
## Feature Detection

#### Convolutional neural networks: 
- Structure designed to mimic perceptual pathways in the human visual system
- Ubiquitous in modern image recognition tasks (Krizhevsky, et al., 2012)

.pull-left[

Pros
- Pre-trained networks available for tuning    
.small[AlexNet, VGG16, ResNet, Inception]
- Features are interpretable
- Very fast (after training)

]

.pull-right[

Cons
- Requires labeled training data
- Computationally expensive to train
- Opaque - parameters are not interpretable

]

???

There are definite advantages to neural networks - they actually work, have been proven to solve much harder problems than this, and provide human-interpretable features because they're built to mimic how the visual cortex works. There are pre-trained networks available, which dramatically reduces the amount of training necessary to get a functional production model. In addition, these were designed to work at google scale, so they can more than meet our computational needs. 

There are disadvantages too - these models require formal training data, which means that someone has to sit stare at the bottom of thousands of shoes, labeling tread features. They're computationally intensive to train, though not all that bad - between 15 minutes and 3 hours during our initial tests on other datasets. Then there's the big problem  - they have so many parameters that it's hard to interpret them, and they're also not designed for actual inference - getting error estimates out of them is difficult.

But, for the moment, "It works" is going to be the more important part than the nicities of simple models with built-in error estimation.

---
class:inv-center
# &lt;br&gt;Convolutional Neural Networks

???

I'm going to really quickly walk through the specific neural network we used for this task and how it (and similar networks) are fit, and why using a pre-trained network is so awesome. Then we'll walk through the results, some model diagnostics, and I'll talk a bit about some of the more interesting issues we've encountered

---
class:primary
## Modeling Approach

Let `\(CNN(x)\)` describe a convolutional neural network acting upon an input image `\(x\)` with labels `\({Y_1}, ..., {Y_z} \in \{0,1\}^z\)` 



`\(CNN(x) \rightarrow [{P_1}, ..., {P_z}] \in [0,1]^9\)` 
- `\(z\)` is the number of output classes 
- `\({P_1}, ..., {P_z}\)` are output class probabilities



The model has errors `\([{\epsilon_1}, ..., {\epsilon_z}] = [{Y_1} - {P_1}, ..., {Y_z} - {P_z}]\)`



---
class:primary
## CNN Architecture

&lt;img src="vgg16-structure/vgg16-shoe-nolabel.png" alt = "Neural Network model structure" width = "95%"/&gt;

???

This is one example of a "very deep" convolutional neural network. Very deep describes the number of convolutional layers (13), which is large compared to other common CNN architectures. The input image, in this case, 256x256, goes through two convolutional layers with 64 filters; it then passes through a max pooling layer where the dimension of the matrix is reduced. In the 128x128 layer, the number of filters used expands, and the filters become somewhat more complex. In each subsequent convolutional block, there are multiple convolutional layers capped off by a dimension-reducing max pooling layer. At the conclusion of the convolutional layers, there is a model head, where the local spatial information identified in the convolutional layers is integrated into unified whole. At the end of the fully connected layers, there is a softmax activation layer that transforms the information into class probabilities. 

I'm going to talk about each of these layer types in turn - it's fairly easy for these models to turn into magic boxes, and we want to avoid that with any model.

---
class: primary
## Image Convolution

Let `\(x\)` be an image represented as a numerical matrix, indexed by `\(i, j\)`, and `\(\beta\)` be a filter of dimension `\(2a + 1 \times 2b + 1\)`

The convolution of image `\(x\)` and filter `\(\beta\)` is `$$(\beta \ast x)(i, j) = \sum_{s = -a}^a\sum_{t = -b}^b \beta(s, t) x(i-s, j-t)$$`

???

Let's start with the convolution part of CNNs. Image convolution is the application of a filter that is smaller than the image to every possible "tile" of the image, where each filter application results in a single value. 

The math is relatively simple, but it's much easier to see what's going on using pictures. Throughout this exercise, we'll refer to the image `\(x\)` and filter `\(\beta\)`; note that the dimension of `\(\beta\)` is odd.

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter.png)

.pull-left[.center[Input image 
`\(\displaystyle x\)`
]]
.pull-right[.center[
Weight matrix
`\(\displaystyle \mathbf{\beta}\)`
]]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

Suppose we have an input matrix and a spatial filter. We'd start by applying that filter to each possible 3x3 portion of the input, which produces a 3x3 output matrix. 

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter1.png)

.pull-left[.center[Convolution: 
`\(\displaystyle \beta\ast x\)`
]]
.pull-right[.center[
Feature Map
`\((\beta \ast x)(i, j)\)`
]]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

We start in the upper left corner of `\(x\)`, applying `\(\beta\)` cell-wise to each overlapping cell of `\(x\)`. We then move over by one and do the same thing.

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter2.png)

.pull-left[.center[Convolution: 
`\(\displaystyle \beta\ast x\)`
]]
.pull-right[.center[
Feature Map
`\((\beta \ast x)(i, j)\)`
]]

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]


???

Some CNNs pad the input image so that the resulting feature map is the same dimension as the input image.

---
class: primary
## Convolutional Layers - &lt;br/&gt;Forward Propagation

- `\(x^{0}(i, j)\)` is an input image of size `\(M\times N\)`

- `\(x^\ell\)`, `\(\ell = 1, ..., n\)` are convolutional layers in the network.

- `\(\beta^\ell_{k}\)` is an `\(m \times m\)` filter matrix in layer `\(\ell\)`, `\(k = 1, ..., p^\ell\)`

- `\(\gamma^\ell\)` is the bias matrix for layer `\(\ell\)`, with the same dimension as `\(\beta^\ell\ast x^{(\ell-1)}\)`

The `\(\ell\)`th layer, `\(x^{(\ell)}\)`, is indexed by `\(i\)`, `\(j\)`, and `\(k\)`:

`$$x^{(\ell)}_k (i, j) = \sigma\left(({\beta^\ell_k}\ast x^{(\ell - 1)})(i, j) + \gamma^\ell\right)$$`

where `\(\sigma(\cdot)\)` is a nonlinear activation function. 


ReLU (Rectified Linear Unit), `\(\sigma(\cdot) = \max\{0, \cdot\}\)` is a common nonlinear activation function


???

Now that the convolutional part is explained, we need to understand how values pass from layer to layer through the network. Forward propagation is the numerical calculation that transitions from image to layer output (and then to the next layer, and so on until the model class probabilities are the output)

We're going to refer to `\(x^0\)` more formally at this point as the input image, and `\(x^\ell\)` as successive layers in the network. We have filters `\(\beta^\ell_k\)`, sometimes called model weights, and biases for each layer, `\(\gamma^\ell\)`. Then our output layer is a three-dimensional matrix, where i and j index the spatial information and k indexes the filters. A nonlinear function is applied to the convolution of the layer and the filter added to the bias matrix. 


---
class:primary
## CNN Architecture

&lt;img src="vgg16-structure/vgg16-shoe-nolabel.png" alt = "VGG16 model structure" width = "95%"/&gt;

???

We've discussed the convolutional layer operations; we'll now talk briefly about the max pooling layers. 

---
class: primary
## Max Pooling Layers

![Pooling Layers](vgg16-structure/maxpooling.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

???

Max pooling layers take a `\(p\timesp\)` region and take the maximum over all cells. Stride is the offset between pooling regions. With a 2x2 window and stride of 2, the output layer is 1/4 the size of the input layer. 

---
class:primary
## CNN Architecture

&lt;img src="vgg16-structure/vgg16-shoe-nolabel.png" alt = "VGG16 model structure" width = "95%"/&gt;

???

We've made it through what's called the model base - the convolutional and pooling layers that aggregate spatial information locally. The model head takes that information and provides a more global integration, culminating in the output of class probabilities after the softmax layer. Different models have different numbers of fully connected layers in the head; the number of layers here has a large influence on the number of parameters that have to be optimized during the fitting process. 

---
class: primary
## Fully Connected Layers
.move-margin[&lt;br&gt;&lt;img src="vgg16-structure/model-head.png" width = "100%"/&gt;]
&lt;img src="20190129-DeptSeminar_files/figure-html/fully-connected-1.png" width="75%" style="display: block; margin: auto;" /&gt;
- Connect every cell of previous layer to every cell of new layer
- Used for spatial pattern integration

???

Fully connected layers are also known as dense layers; they take the spatial information and look for patterns. These fully connected layers often have "dropout", which reduces the number of parameters by pruning weak connections.

---
class: primary
## Dropout Layers
.move-margin[&lt;br&gt;&lt;img src="vgg16-structure/model-head.png" width = "100%"/&gt;]
&lt;img src="20190129-DeptSeminar_files/figure-html/dropout-layers-1.png" width="75%" style="display: block; margin: auto;" /&gt;
- Reduces number of parameters
- Used for spatial pattern integration

???

Here's a representation of a layer that's fully connected, and the same layer that has a 50% dropout rate.


---
class: primary
## Fitting Mechanism

- Forward Propagation: Input -&gt; Filters -&gt; Pooling -&gt; Result

- Backward Propagation: Errors -&gt; Pooling -&gt; Filters to update weights
    - Loss function `\(L\)` describing the prediction errors
    - Gradient descent using `$$\frac{\partial L}{\partial \beta}$$`
    at iteration `\(t\)`, with learning rate `\(\lambda\)`, 
    `$${\beta_k}(t) = \beta_k(t-1) - \lambda \frac{\partial L}{\partial {\beta_k}}$$`
    
???

During forward propagation, we begin with an input image and, for each layer, use the filters and pooling operators to compute a series of features that contribute to the resulting output probability. 

To fit the model to the training data, we must be able to move backwards, adjusting the filter weights and biases in order to produce better estimates. Backward propagation, or backpropagation, is how this adjustment occurs. In essence, CNN backpropagation employs gradient descent, applied to each cell of each layer. 

Common loss functions are squared error loss and cross-entropy, depending on the classification type and specific output layer.  

---
class: primary
## Backward Propagation

`$$\begin{align}\left(\frac{\partial L}{\partial \beta^\ell_k}\right) &amp;= \underbrace{\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)(i,j)}}_\text{gradient} x^{\ell-1}_{ij}\\\\\frac{\partial L}{\partial \left(\beta^\ell_k \ast x^{\ell - 1}\right)(i,j)}&amp;=\frac{\partial L}{\partial x_{ij}^\ell} \left[\sigma'\left(\beta^\ell_k \ast x^{\ell - 1}\right)(i,j)\right] \end{align}$$`

The gradient can be computed with the derivative of the activation function `\(\sigma(\cdot)\)`

&lt;br/&gt;
.small[Note: ReLU is not differentiable at 0; common practice is to set 0, 0.5, or 1 as the derivative's value at 0.]

???

I'm going to talk about the backpropagation of the weights and ignore the biases for the time being; the process is even simpler for the biases because they are not convolved with the previous layer. 

Very simply, the gradient used to update the weights can be computed using the derivative of the activation function. 

---
class: primary
## Backward Propagation

To propagate errors to the previous layer, 

`$$\begin{align}\frac{\partial L}{\partial x_{ij}^{\ell -1}} &amp;= \frac{\partial L}{\partial \left(\beta_k^\ell \ast x^{\ell - 1}\right)(i,j)} \beta_k^\ell\end{align}$$`

&lt;br/&gt;
.center[&lt;img src="vgg16-structure/vgg16-shoe-nolabel.png" width = "50%"/&gt;]

.center[&lt;b&gt;13 convolutional layers = a lot of backpropagation&lt;/b&gt;]

???

Then, with a bit more manipulation, we can move the errors from the current layer to a previous layer, and use that to update weights there.

With 13 convolutional layers, and a 256x256 image, this process is repeated a lot. It's only relatively recently that the computational power required to fit neural networks with this complexity became available, though the process has been around since the 1980s.

---
class: primary
## Parameter Space

.move-margin[&lt;br/&gt;![VGG16 model structure](vgg16-structure/vgg-sideways-nohead.png)]
- Convolutional base: ~14.5 million parameters

- Simple model head (9 output classes): ~8.4 million parameters

- Total parameter space: ~22.9 million

- Estimated model optimization time: 2-3 weeks with 4 GPUs

- Data requirements: &gt; 1 million labeled images

--

&lt;br/&gt;&lt;br/&gt;

.center[We have 25k labeled images]

???

These networks are so powerful in part because they have so many parameters; a total of 22.8 million for a model with the convolutional structure we've been working with and a very simple model head. That many parameters requires an amount of labeled training data we simply don't have; the time to fit those models is also unappealing: around a month using multiple GPUs. 

---
class: primary
## Transfer learning

.move-margin[&lt;br/&gt;![VGG16 model structure](vgg16-structure/vgg-sideways-connorhead.png)]

- Use weights from a model trained on different input data

- Freeze the weights in the convolutional base

- Train a new model head

- Total parameter space: 8.4 million

- Model optimization time: &lt;3 hours

--

#### VGG16
- Pre-trained CNN (Simonyan, et al., 2014)
    - Trained on 1.3 million images from ImageNet    
    (Krizhevsky, et al., 2012)
    
    - Simple structure

???

We can use transfer learning to take advantage of the fact that there are pre-trained models available: we use the convolutional base (and the weights) from the pre-trained model and fit a new model head using the data we do have. This saves a huge amount of computational time and takes advantage of the fact that visual features generalize fairly well; the same set of filters that can recognize cats and dogs can also recognize circles and squares if the dense layers are properly calibrated. Using this approach, the model takes less than 3 hours to fit (we run model updates every night). 

---
class: inv-center
# Fitting CoNNOR: Convolutional Neural Network for Outsole Recognition

???

Now that you have an idea of what's happening inside the computer, we're going to focus on the more human side of the equation.

---
class:primary
## Acquire Data
&lt;img alt="Zappos Screenshot showing sole images" src="labelme-imgs/zappos.png" width="90%" style="margin: 0 5%"/&gt;

.move-margin[&lt;br/&gt;&lt;br/&gt;50237 images scraped since April 2018]

???

To start with, where'd the data come from? I built a scraper to pull images from Zappos; the images are relatively high resolution (usually 1920x1400). It's been running since late April, and we have about 50K images at this point.

The next step was to get humans to label things in the same way we want the model to label things.

---
class:primary
## Label Data
![LabelMe](labelme-imgs/LabelMe1.png)

- [LabelMe Annotation Tool](https://github.com/CSAILVision/LabelMeAnnotationTool) used as a web interface - creates XML files with labels and coordinates. (Russell, et al., 2008)

- 24,954 regions labeled with one or more geometric objects
- 33,305 labels

.small[.move-margin[
&lt;br/&gt;&lt;br/&gt;Labeling courtesy of 
- Jenny Kim
- Ben Wonderlin
- Mya Fisher
- Holden Jud
- Miranda Tilton
- Charlotte Roiger
- Joe Zemmels
- and others
]]

???

The real heroes here are the high schoolers, undergraduates, and grad students who spent time labeling images to provide training data for the model. 

One issue we ran into is that our criteria changed as we found new edge cases that forced us to make a decision on whether something was part of a specific category. In addition, initially we were using the Matlab toolkit that came with the LabelMe program to process and extract the images from the annotation files. Over the summer, I rewrote that code in R to customize it to our needs. This rewrite changed some of the previous guidelines - instead of labeling multiple features that overlap separately, we now label all of the features within a region with the same polygon. This reflects a change in how modeling occurs as well as a change in the processing code. Unfortunately, we're still cleaning up the annotations that were completed before that point. 

---
class:primary
## Label Data
&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/label-data-barchart-1.png" alt="Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon." width="90%" /&gt;
&lt;p class="caption"&gt;Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon.&lt;/p&gt;
&lt;/div&gt;

???

This graph reflects the current state of class labels. There are far more quadrilaterals, lines, and text than any other category; quads in particular are more likely to appear alone than in multiple groups. Stars, polygons, and bowties are much less likely to occur; this large discrepancy in class frequency does make modeling more interesting. 

---
class:primary
## Model Specification
Multiple classes, multiple labels: "One-hot" encoding

Statistically: 

- Model output: `\((P_1, ..., P_9) \in [0,1]^9\)`
    - Each geometric feature assigned a probability
    - An image can be labeled with multiple features

- Output probabilities `\(P_i\)` are not independent
    - Dependencies due to CNN structure
    - Dependencies due to input data
    - Dependencies due to geometric similarity -     
    Polygons vs. Quadrilaterals
    
- Covariance structure is ?

--

.move-margin[&lt;br/&gt;&lt;br/&gt;There are downsides to black-box models...]

???

We've set the model up so that it predicts the presence or absence of each of 9 classes. In machine learning, this is "one-hot" encoding, but you may be more familiar with it as coding categories using indicator variables. 

What's important to note is that because any given image can have between 0 and 9 class labels, each of the 9 probabilities produced by the model are between 0 and 1, but there are no other constraints - they do not have to sum to 1. 

We can think of this as a multivariate bernoulli situation, where each label is assigned with probability `\(\pi_i\)`. The `\(pi\)`s are not independent - there is obviously a dependence both due to feature similarity (e.g. polygons and quads have similar features) and input data - easily identifiable classes are more likely to have been labeled. There's also a dependency due to the CNN structure, but I'd be hard pressed to describe it beyond saying that it exists.

So any attempts to think too hard about the covariance structure are (at this point) doomed. There are downsides to using a black-box model; one of those is that statistical formalities are not necessarily observed. Of course, the upside is that the model actually works.


---
class:primary
## Model Training

- 256 x 256 pixel images

- Training data (60%):
    - 1x Augmented images (rotation, skew, zoom, crop) to prevent overfitting
    - Class weights used to counteract uneven class sizes
    
- Validation and test data (20% each)

- Miranda Tilton fit the model using the `keras` package in R, which provides a high-level API for the `tensorflow` library 

???

We scaled all of the labeled images to 256 x 256; aspect ratio was not preserved, though some steps have been taken to ensure that the labeled regions are at least square-ish where possible to prevent extreme distortion. 

60% of the labeled images were used as training data; these images were augmented once by zooming, skewing, cropping, and rotating the images. This step is recommended to prevent over-fitting, and is particularly important given that many features (e.g. bowties in Birkenstocks) are similar across shoes, so an image that is essentially the same may appear in the training, validation, and test sets.

Validation data, which is used within each fitting iteration to calculate the loss function, accounted for 20% of the images, and test data, which is used to evaluate the model at the end of the fitting process, accounted for the remaining 20%. 

We used the keras package in R to fit the model using the tensorflow toolkit. Tensorflow is an extremely efficient implementation that can use either the CPU or GPU to fit the neural network. It was originally developed by Google's Machine Intelligence team. Keras makes it easy to use VGG16, remove the model head, freeze the weights on the base, and add a new head, using only a few lines of code. 

---
class:primary
## Model Training

&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/training-accuracy-1.png" alt="Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy." width="99%" /&gt;
&lt;p class="caption"&gt;Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.&lt;/p&gt;
&lt;/div&gt;

.move-margin[&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;.small[Binary Cross-entropy Loss: 
`$$-y\log(p)\\\\-\\!(1\\!-\\!y)\log(1\\!-\\!p)$$`
]]
???

This chart shows model performance relative to the loss and accuracy rate during each epoch (backpropagation occurs after each epoch of fitting). 

Validation loss levels off after 15 epochs, but hasn't yet begun to increase. Training loss is still decreasing as well. One concern with retraining the head of a CNN is that with relatively little data (e.g. 20 thousand data points instead of 150K) it is easy to over-fit models; what we see is that this hasn't yet happened for this model. Overfitting would be evident if the loss in the training set had beun to increase. 

---
class:primary
## Evaluating the Model
&lt;img src="20190129-DeptSeminar_files/figure-html/overall-roc-1.png" width="100%" /&gt;

???

We can compute an aggregate ROC curve that treats all classes the same. Under this, we see that performance is generally fairly good, though there is obviously room for improvement. The more interesting evaluation is to look at prediction accuracy for each label...

---
class:primary
## Evaluating the Model
&lt;!-- Add in model overall AUC --&gt;
&lt;!-- Describe the multi-class version as splitting out model performance by class --&gt; 

&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/class-roc-1.png" alt="Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class." width="99%" /&gt;
&lt;p class="caption"&gt;Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class.&lt;/p&gt;
&lt;/div&gt;

???

These plots show ROC curves for each class, computed separately. Because this problem is a multivariate binary classification problem (in statistical parlance), we can adapt most of the diagnostic plots we're used to with a few modifications. In this case, each ROC curve was generated without consideration of other plot features.

Equal error rates are marked with a dot, and show the point at which it is equally likely for the model to miss a classification or wrongly classify an image. These EERs are used as an optimized cutoff value for diagnostics which require a hard threshold, like a confusion matrix.

---
class:primary
## Evaluating the Model
&lt;div class="figure"&gt;
&lt;img src="20190129-DeptSeminar_files/figure-html/ConfMatrix-1.png" alt="Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes." width="80%" /&gt;
&lt;p class="caption"&gt;Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes.&lt;/p&gt;
&lt;/div&gt;

.move-margin[

&lt;br/&gt;
For multi-label images, only incorrect predictions contribute to off-diagonal probabilities

`\(EER_i\)` used as the cutoff
&lt;br/&gt;

]

???

This confusion matrix shows, for each label, the probability that the image is classified as that label as well as other possible labels. One modification we made to the standard confusion matrix was to exclude any additional "correct" labels from these calculations: If an image was labeled with a circle and a line, but the model assigned circle and triangle as labels, then in the circle column that image would register as a true positive for circle and a false positive for triangle; line would be excluded from calculations in that column. Similarly, in the line column, triangle would be marked as a false positive, circle would be excluded, and line would be marked as a false negative. 

An important point to make at this juncture is that while we're operating as if our labeled data were "ground truth", that isn't an accurate assumption. People make mistakes, labeling is monotonous, and the criteria for certain classes have changed over time. In some cases, the model is correct, and the labels are wrong. We're working on correcting the labeling, but even in a situation where the labeling is done in accordance with the guidelines, some of the criteria can get fuzzy in practice. 

---
class:primary
## Definitions matter
![Classes get confusing](labelme-imgs/dc_circle_quad_confusion.png)

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

--

![Not everything is labeled correctly](labelme-imgs/adidas_circle_pred_correct.png)

???

We created a shiny application to see the images and the model's predictions. Blue means that the image had that label, grey means it does not. 

So in the first image, the design is labeled as a quadrilateral (rounded quadrilateral) rather than a circle. We're still debating how to handle these... they're very ambiguous figures with features of both circles and quadrilaterals. The model is not wrong, but neither is the labeling... or maybe both of them are wrong?

In the second image, both text and quad are labels, but the model also identifies a circle in the text with probability 0.31; that is, the text contains an O. 

We use the shiny application to screen for these problems so that we can correct the labeled training data where it does not meet the current guidelines for labeling. When we find an issue that's widespread (e.g. the Adidas logo is often not labeled as containing quads) we examine the shoes matching that description (so some screening occurs outside of what is predicted by the model). 

We're trying to ensure that the data used to train the model is of very high quality, while not spending millions of dollars to hire workers online to label things. Because we determined the guidelines for labeling the data, labeled the data (or oversaw the labeling), and trained the model ourselves, we have the advantage of knowing the flaws at every point in the process; that means we have the responsibility to fix those flaws where possible. 

We're not doing inference on the model results at this point (nor planning to use the data we're training the model with during the operational stage) so the data -&gt; model -&gt; fix data loop is less of a validity concern. 

When the model is sufficiently well-calibrated, we can then work with engineers to build the device, collect some initial data, and tweak the model weights with new data that better represents what we'll actually see from the collection equipment. By that point, hopefully we'll also have narrowed down the geometric classification scheme so that categories that are now somewhat fuzzy are more clearly operationalized.


---
class:primary
## Interpreting the model

What is a CNN actually doing? (Olah, et al., 2018)

1. Which regions in the image are relevant to each class?

2. Which regions in the image activate which filters?

3. Which filters are most important for detection of each class?
    
4. What do the filters detect?
    - Semantic segmentation (Long, et al., 2015)
    - Filter visualization

&lt;div class="small move-margin"&gt;
&lt;!-- 2. &lt;b&gt;Regions and Filters:&lt;/b&gt; --&gt;
&lt;img src="interpretation/cat_dog.png"/&gt;
&lt;img src="interpretation/cat_dog_filters.png"/&gt;
&lt;span style="font-size:42%"&gt;Source: https://distill.pub/2018/building-blocks/&lt;/span&gt;
&lt;/div&gt;

&lt;div class="pull-left"&gt;
&lt;img src="interpretation/conv1_filters.png" alt="VGG16 Filters, convolutional layer 1"/&gt;
&lt;span style="font-size:30%"&gt;Source: https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html&lt;/span&gt;
&lt;/div&gt;

&lt;div class="pull-right"&gt;
&lt;img src="interpretation/neuron_attribution.png" width = "74%" style = "margin-left:13%;margin-right:13%;" alt="Which regions and filters?"/&gt;
&lt;span style = "margin-left:13%;margin-right:13%;font-size:30%"&gt;Source: https://distill.pub/2018/building-blocks/&lt;/span&gt;
&lt;/div&gt;
???

During this process, it's also helpful to see what the model is using to determine which features are present or absent. Black box models are notoriously difficult to see inside, but this is a very active area of current research. 

In particular, we'd like to know which regions in the image are relevant to the class, and which filters in the model are important for detecting each class. We've made some progress on the first question, and are actively looking into the second. 





---
class:primary
## Interpreting the model&lt;br&gt;Class Activation Maps

&lt;br/&gt;
&lt;br/&gt;
![unscaled heatmapp - DC](heatmaps/heatmap-quad-4-dc-pure-se-navy_product_7270757_color_9.png)
&lt;br/&gt;
&lt;br/&gt;
Heatmaps are scaled by class. Yellow = high activation

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

Class activation maps use the gradient with respect to each class label, back propagated to the last convolutional layer to maintain spatial information. Here, we can see that the D shape discussed previously is activating both circle and quad; the round part is activating the circle class and the straight part is activating the quad class; there's also some slight activation of the polygon class, which isn't surprising given that polygons and quads share some features. 

---
class:primary
## Interpreting the model&lt;br&gt;Class Activation Maps

&lt;br/&gt;
&lt;br/&gt;
![unscaled heatmapp - adidas](heatmaps/heatmap-test_image.png)
&lt;br/&gt;
&lt;br/&gt;
Heatmaps are scaled by class. Yellow = high activation

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

This is a piece of the Adidas text that is part of the design of many of their shoe treads. The model is obviously cuing into the text, but interestingly, it also detects circles - the text is so round that the inside of any A or D character is a perfect circle. While we wouldn't necessarily go out of our way to label that, because text dominates the human brain, we've created a model that doesn't have that dominance; the circles are just as real as the text to this model, and should be labeled as such.


---
class:primary
## Interpreting the model&lt;br&gt;Class Activation Maps

&lt;br/&gt;
&lt;br/&gt;
![unscaled heatmapp - seychelles](heatmaps/heatmap-text-2-seychelles-slow-down-blush-metallic_product_9017725_color_34700.png)
&lt;br/&gt;
&lt;br/&gt;
Heatmaps are scaled by class. Yellow = high activation

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

???

In a similar instance, the model cues in on circles in script S characters as well. These are objectively not circles, though, as they aren't closed, so we do not update the labeling for these images. It's a delicate balance between "the model isn't wrong" and tweaking the labels to match the model output exactly - we're working to find that balance. 

---
class:primary
## Summary

- Geometric shapes provide a convenient feature space for assessing shoe similarity

- Transfer learning allows application of CNNs to much smaller datasets

- CoNNOR performs well
    - Reduction in feature space: 256 x 256 x 3 -&gt; 9
    
    - 88% accuracy; many errors attributable to data labeling




---
class:inv-center
# &lt;br/&gt;&lt;br/&gt;What's Next?

---
class: primary
## What's Next?

- Paper &lt;!--where?--&gt;

- NIJ Grant to build the machine and collect local population data

- Make CoNNOR more interpretable
    - keras-vis Python library -&gt; R package 
    
- Whole-Shoe predictions - incorporate spatial information


---
class: inv-center
# Questions?

---
class: primary
## References
&lt;div class="small"&gt;

&lt;p&gt;[1]&lt;cite&gt;
I. Benedict, E. Corke, R. Morgan-Smith, et al.
&amp;ldquo;Geographical variation of shoeprint comparison class correspondences&amp;rdquo;.
In: &lt;em&gt;Science and Justice&lt;/em&gt; 54.5 (2014), pp. 335&amp;ndash;337.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;cite&gt;
S. Gross, D. Jeppesen and C. Neumann.
&amp;ldquo;The variability and significance of class characteristics in footwear impressions&amp;rdquo;.
In: &lt;em&gt;Journal of Forensic Identification&lt;/em&gt; 63.3 (2013), p. 332.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[3]&lt;cite&gt;
A. Krizhevsky, I. Sutskever and G. E. Hinton.
&amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo;.
In: 
&lt;em&gt;Advances in Neural Information Processing Systems 25&lt;/em&gt;.
Ed. by F. Pereira, C. J. C. Burges, L. Bottou and K. Q. Weinberger.
Curran Associates, Inc., 2012, pp. 1097&amp;ndash;1105.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[4]&lt;cite&gt;
J. Long, E. Shelhamer and T. Darrell.
&amp;ldquo;Fully Convolutional Networks for Semantic Segmentation&amp;rdquo;.
In: 
2015, pp. 3431&amp;ndash;3440.
URL: &lt;a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html"&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[5]&lt;cite&gt;
C. Olah, A. Satyanarayan, I. Johnson, et al.
&amp;ldquo;The Building Blocks of Interpretability&amp;rdquo;.
En.
In: &lt;em&gt;Distill&lt;/em&gt; 3.3 (Mar. 2018). 00079, p. e10.
ISSN: 2476-0757.
DOI: &lt;a href="https://doi.org/10.23915/distill.00010"&gt;10.23915/distill.00010&lt;/a&gt;.
URL: &lt;a href="https://distill.pub/2018/building-blocks"&gt;https://distill.pub/2018/building-blocks&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[6]&lt;cite&gt;
B. C. Russell, A. Torralba, K. P. Murphy, et al.
&amp;ldquo;LabelMe: A Database and Web-Based Tool for Image Annotation&amp;rdquo;.
En.
In: &lt;em&gt;International Journal of Computer Vision&lt;/em&gt; 77.1-3 (May. 2008). 02464, pp. 157&amp;ndash;173.
ISSN: 0920-5691, 1573-1405.
DOI: &lt;a href="https://doi.org/10.1007/s11263-007-0090-8"&gt;10.1007/s11263-007-0090-8&lt;/a&gt;.
URL: &lt;a href="http://link.springer.com/10.1007/s11263-007-0090-8"&gt;http://link.springer.com/10.1007/s11263-007-0090-8&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[7]&lt;cite&gt;
K. Simonyan and A. Zisserman.
&amp;ldquo;Very Deep Convolutional Networks for Large-Scale Image Recognition&amp;rdquo;.
En.
(Sep. 2014).
URL: &lt;a href="https://arxiv.org/abs/1409.1556"&gt;https://arxiv.org/abs/1409.1556&lt;/a&gt;.&lt;/cite&gt;&lt;/p&gt;
&lt;/div&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
