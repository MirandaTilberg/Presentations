<!DOCTYPE html>
<html>
  <head>
    <title>CoNNOR: Convolutional Neural Network for Outsole Recognition</title>
    <meta charset="utf-8">
    <meta name="author" content="Susan VanderPlas" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="css/csafe.css" type="text/css" />
    <link rel="stylesheet" href="css/csafe-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# CoNNOR: Convolutional Neural Network for Outsole Recognition
### Susan VanderPlas
### Jan 29, 2019

---

class: primary





class:primary
## Outline

- Defining the Problem

- Class Characteristics in Footwear

- Image Analysis Methods

- Convolutional Neural Networks

- CoNNOR

- Future Work

---
class:inv-center
# What is the probability I made this shoeprint?

---
class:primary
### &lt;br/&gt;What is P(someone else made this print)?

1. Define the comparison population

2. Sample from the comparison population

3. Identify similar shoes from the comparison population

4. Calculate probability: `$$\frac{\# \text{ similar shoes}}{\text{Size of comparison population}}$$`

???

Comparison population definition could include various size geographic regions, temporal components, interaction with local events - shoes worn by people attending a football game likely differ from shoes worn by people at a formal event. Local areas range from state-level to "People outside Starbucks in CampusTown at 8am on a Monday morning"

---
class:primary,center,middle
## Comparison Population

&gt; .large[Quantifying the frequency of shoes in a local population is an unsolveable problem]&lt;br/&gt; - Leslie Hammer, March 2018


---
class:primary
## Comparison Population

- No 100% complete database of all shoes 
    - manufacturer, model, size, tread style, manufacturing molds [Ham89]
    
- Shoe purchase data vs. frequency of wear

- Local populations may differ wildly .small[[Ben+14]]

&lt;br/&gt;&lt;br/&gt;
.center[&lt;img src = "problem-definition/snow-boots.jpg" width = "50%" style = "vertical-align:middle;float:middle"/&gt;]
&lt;!-- https://pixnio.com/free-images/2017/05/03/2017-05-03-07-35-18-900x456.jpg --&gt;

???

There are too many manufacturers to keep track of, new models are released all the time, custom shoe markets, and knockoff shoes... hard to get a database with everything.

In addition, many of us have shoes that we've purchased but almost never wear... so going off of purchase data (even if that had the geographic resolution needed) is not all that representative

Finally, geographic resolution is nearly impossible. If this pair of shoes was used to commit a crime in Florida, it would be fairly damning to have the same type of shoes in your closet - they aren't "Florida" shoes. If you have this pair of shoes in your closet in Iowa right now, the weight of that evidence is a lot lower, because they're probably fairly common shoes to have. 

---
class:primary
## Comparison Population&lt;br&gt;Shoes

How to collect data from the local population? 

1. Build a low profile scanner that can be placed in a high traffic area

2. Scan shoes of those walking past

3. Create a local-area database of relevant scans

--

### .center[This is an engineering problem]

---
class:primary
## Comparison Population&lt;br&gt;Shoes

Assume a machine exists that can scan shoe outsoles of pedestrians

--

1. Identify relevant features within the scans

--

2. Assess the frequency of similar shoes in the local-area database

--

### .center[These are statistics and machine learning problems]

---
class:primary
## Relevant Features

Class Characteristics
- Make, Model, Tread pattern, Size, Type of shoe

- Cannot be used to identify an individual match

- Used for exclusion

---
class:primary
## Relevant Features

Use features other than make/model to characterize shoes

- Knockoffs often have very similar tread patterns
- Similar styles have similar tread patterns across brands
- Unknown shoes can still be classified and assessed

&lt;img src = "problem-definition/star_bar_drmartin.png" width = "16%" style = "vertical-align:middle;padding-left:60px;padding-right:30px"/&gt;
&lt;img src = "problem-definition/star_bar_vibram.png" width = "15%" style = "vertical-align:middle;padding-left:30px;padding-right:30px"/&gt;
&lt;img src = "problem-definition/star_bar_timberland.png" width = "16%" style = "vertical-align:middle;padding-left:30px"/&gt;

---
class:primary
## Relevant Features

| Bowtie | Chevron | Circle |
| ------ | ------- | ------ |
| ![Bowtie examples](class_examples/bowtie_examples.png) | ![Chevron examples](class_examples/chevron_examples.png) | ![Circle examples](class_examples/circle_examples.png) |

| Line | Polygon | Quadrilateral |
| ---- | ------- | ---- |
| ![Line examples](class_examples/line_examples.png) | ![Polygon examples](class_examples/polygon_examples.png) | ![Quad examples](class_examples/quad_examples.png) |

| Star | Text | Triangle |
| ---- | ---- | -------- |
| ![Star examples](class_examples/star_examples.png) | ![text examples](class_examples/text_examples.png) | ![Triangle examples](class_examples/triangle_examples.png) |

Used to separate shoes by make/model in (small) local samples [GJN13]
---
class: inverse
# &lt;br/&gt;Image Analysis and Feature Detection

---
class: primary
## Image Analysis

### Goal: Identify geometric tread features in images of shoe outsoles

- Robust to different lighting conditions, rotation, image quality

- Fast processing of new images

- Identify features that are explainable to practitioners

---
class:primary
## Feature Detection

#### Classic computer vision feature detection methods: 

- Edge, Corner, Blob, Ridge detection

- Template matching: Hough transforms 
    - line, circle, ellipse detection
    - provide location and orientation

--

.pull-left[
Pros
- No training data necessary
- Relatively simple algorithm
]

--

.pull-right[
Cons
- Not robust (fragile tuning parameters)
- Computationally intensive
- Features lack face validity
]

???

There are some computer vision methods that detect some of the geometric features we've identified. Edge and corner detection are used in image matching, blob detection is used everywhere from biology (identifying cells, nuclei, tumors) to astrophysics (galaxies and stars), and ridge detection is used to identify topographic features in images. More complicated features, such as lines, circles, and ellipses can be detected with Hough transforms, which are a template matching algorithm that's very computationally intensive. 

I spent some time trying to assemble some of these detectors to work with shoe outsole images and found that they were not robust - parameter settings that worked well for one image failed miserably when used on the same model of shoe with a slightly different color sole, the algorithms were slow, and the features they picked out were often not what I would call an edge or a corner - the algorithm was working at a much finer level than I was. As all of our work has to be explained to practitioners eventually, that made these methods sub-optimal. Google, Facebook, and many others have used convolutional neural networks to solve the image recognition problem in the past 10 years, so I moved on to the big guns. 

---
class:primary
## Feature Detection

#### Convolutional neural networks: 
- Structure designed to mimic perceptual pathways in the human visual system
- Ubiquitous in modern image recognition tasks

--

.pull-left[

Pros
- Pre-trained networks available for tuning    
.small[AlexNet, VGG16, ResNet, Inception]
- Features are interpretable
- Very fast (after training)

]

--

.pull-right[

Cons
- Requires labeled training data (lots!)
- Computationally intensive to train
- Opaque - parameters are not interpretable

]

---
class:inverse
# Convolutional Neural Networks







&lt;!-- Add in information about what the layers are doing: https://www.overleaf.com/project/5bef1722f138c36ed9d572d4 --&gt;
---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

.small[VGG16 was trained on an ImageNet [KSH12] dataset of 150,000 images with 1000 output classes [SZ14]]

---
class: primary
## Convolutional Layers

For layer `\(\ell\)` and cell `\(ij\)`, with weights `\(\mathbf{w}\)` 

`$$x^\ell_{ij} = \sum_{a=0}^{m-1}\sum_{a=0}^{m-1} w_{ab} y_{(i+a)(j+b)}^{\ell-1}$$`

The  nonlinear activation function `\(\sigma(\cdot)\)` is then applied: `\(y_{ij}^\ell = \sigma(x_{ij}^\ell)\)` 

--

This is "Forward Propagation"

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]
---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter1.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

---
class: primary
## Convolutional Layers

![Input image and filter](vgg16-structure/filter2.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

--

.move-margin[&lt;br/&gt;&lt;br/&gt;
The activation function is applied to the feature map values
]

---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

---
class: primary
## Max Pooling Layers

![Pooling Layers](vgg16-structure/maxpooling.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]

---
class:primary
## CNN Architecture - VGG16

&lt;img src="vgg16-structure/vgg16-shoe.png" alt = "VGG16 model structure" width = "95%"/&gt;

---
class: primary
## Densely Connected Layers

.img75[![Dense layers](vgg16-structure/densely_connected.png)]

---
class: primary
## Dropout Layers

![Dropout layers](vgg16-structure/dropout.png)

.footer[Image source: https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2]


---
class: primary
## Fitting Mechanism

- Forward Propagation: Input -&gt; Filters -&gt; Pooling -&gt; Result

- Backward Propagation: Errors -&gt; Pooling -&gt; Filters to modify weights
    - Loss function `\(L\)` describing the prediction errors
    - Compute `\(\displaystyle\left(\frac{\partial L}{\partial y_{ij}^\ell}\right)\)` for each cell in the previous layer
    

`$$\left(\frac{\partial L}{\partial w_{ab}}\right) = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} \frac{\partial x^\ell_{ij}}{\partial w_{ab}} = \sum_{i = 0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial L}{\partial x^{\ell}_{ij}} y^{\ell-1}_{(i+a)(j+b)}$$`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]
---
class: primary
## Backward Propagation

`\(\displaystyle \frac{\partial L}{\partial x^{\ell}_{ij}}\)` is the gradient component:

`$$\frac{\partial L}{\partial x^{\ell}_{ij}} = \frac{\partial L}{\partial y_{ij}^\ell}\frac{\partial y_{ij}^\ell}{\partial x_{ij}^\ell} = \frac{\partial L}{\partial y_{ij}^\ell} \frac{\partial }{\partial x_{ij}^\ell}\left(\sigma(x_{ij}^\ell)\right) = 
 \frac{\partial L}{\partial y_{ij}^\ell} \sigma'(x_{ij}^\ell)$$`

`\(\displaystyle\frac{\partial L}{\partial y_{ij}^\ell}\)` is error at the current layer

The gradiant can be computed with the derivative of the activation function `\(\sigma(x)\)`

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]
---
class: primary
## Backward Propagation

&lt;br/&gt;
To propagate errors to the previous layer, 

`$$\frac{\partial L}{\partial y_{ij}^{\ell -1}} = \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell}\frac{\partial x_{(i-a)(j-b)}^\ell}{\partial y_{ij}^{\ell-1}}
= \sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial L}{\partial x_{(i-a)(j-b)}^\ell} \omega_{ab}$$`

&lt;br/&gt;
.center[&lt;img src="vgg16-structure/vgg16-shoe.png" width = "50%"/&gt;]

.center[&lt;b&gt;13 convolutional layers = a lot of backpropagation&lt;/b&gt;]

.footer[Source: http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/]

---
class: primary
## VGG16

&lt;br/&gt;
- Convolutional base of VGG16 for 256 x 256 x 3 images has __14,714,688__ parameters

- Head has __155,090,944__ parameters and outputs __1000__ different class label predictions, fit using __150,000__ images on 

--

- We have __9__ class labels and __&lt;25,000__ labeled images

- Solution: Use the pre-trained VGG16 base (all of the convolutional layers) and train a new head
    - Reduces the parameter space to __8,391,177__ parameters

???

Equivalent of putting a new "brain" on a set of "eyes" that's pre-trained

---
class: inverse
# Fitting CoNNOR: Convolutional Neural Network for Outsole Recognition

---
class:primary
## Acquire Data
&lt;img alt="Zappos Screenshot showing sole images" src="labelme-imgs/zappos.png" width="90%" style="margin: 0 5%"/&gt;

.move-margin[&lt;br/&gt;&lt;br/&gt;49745 shoe outsole images scraped since June 2018]

---
class:primary
## Label Data
![LabelMe](labelme-imgs/LabelMe1.png)

- 31,496 shoe images labeled

- 23,898 regions labeled with one or more geometric objects

--
.small[.move-margin[
&lt;br/&gt;&lt;br/&gt;Labeling courtesy of 
- Jenny Kim
- Ben Wonderlin
- Holden Jud
- Mya Fisher
- Miranda Tilton
- Charlotte Roiger
- and others
]]

---
class:primary
## Label Data
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/unnamed-chunk-5-1.png" alt="Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon." width="90%" /&gt;
&lt;p class="caption"&gt;Distribution of classes in all labeled images. Quadrilaterals, lines, circles, text, and chevrons are relatively common; stars, polygons, and bowties are relatively uncommon.&lt;/p&gt;
&lt;/div&gt;

---
class:primary
## Model Specification
Multiple classes, multiple labels: "One-hot" encoding

Statistically: 

- Model output: `\((P_1, ..., P_9) \sim Bernoulli(\mathbf{\pi})\)`
    - Each geometric feature assigned a probability
    - An image can be labeled with multiple features

- Output probabilities are not independent
    - Dependencies due to CNN structure
    - Dependencies due to input data
    - Dependencies due to geometry -     
    Polygons vs. Quadrilaterals vs. Lines
    
- Covariance structure is ?

--

.move-margin[&lt;br/&gt;&lt;br/&gt;There are downsides to black-box models...]

---
class:primary
## Model Training

&lt;div style="margin-left:-20px"&gt;
- 256 x 256 pixel images

- Training data (60%):
    - 1x Augmented images (rotation, skew, zoom, crop) to prevent overfitting
    - Class weights used to counteract uneven class sizes
    
- Validation and test data (20% each)

- Fit using the `keras` package in R, which provides a high-level API for the `tensorflow` library .small[[CA18]]
&lt;/p&gt;

---
class:primary
## Model Training

&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/training-accuracy-1.png" alt="Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy." width="99%" /&gt;
&lt;p class="caption"&gt;Training and Validation accuracy and loss for each epoch of the fitting process. Training and validation accuracy reach 90% around epoch 14. After that point, validation loss remains about the same and training loss decreases slightly, while validation accuracy increases more slowly than training accuracy.&lt;/p&gt;
&lt;/div&gt;

???

Validation loss levels off after 15 epochs, but hasn't yet begun to increase. Training loss is still decreasing as well. One concern with retraining the head of a CNN is that with relatively little data (e.g. 20 thousand data points instead of 20 million) it is easy to over-fit models; what we see is that this hasn't yet happened for this model. 

---
class:primary
## Evaluating the Model
&lt;!-- Add in model overall AUC --&gt;
&lt;!-- Describe the multi-class version as splitting out model performance by class --&gt; 

&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/unnamed-chunk-6-1.png" alt="Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class." width="99%" /&gt;
&lt;p class="caption"&gt;Receiver Operating Characteristic curves for the 9 classes used to fit CoNNOR, generated individually for each class.&lt;/p&gt;
&lt;/div&gt;

???

As we discussed previously, this is a multivariate classification problem, in that there are 9 separate output probabilities. I've adapted standard ways of evaluating machine learning model performance to this problem, but it should be noted at this point that the graphs I'm showing here are improvised - ROC curves typically aren't used in classification problems with multiple classes, but once multiple labels are allowed, the problem (somewhat) reduces back to a set of 9 binary classification problems. Here, each ROC curve was generated without considering other features. 

The ROC curves shown here are calculated for each geometric feature without considering the other features. Multi-class ROC curves exist, but do not allow for multi-label classification, so instead separate calculations were performed for each class independently. 

---
class:primary
## Evaluating the Model
&lt;div class="figure"&gt;
&lt;img src="20180129-DeptSeminar_files/figure-html/ConfMatrix-1.png" alt="Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes." width="80%" /&gt;
&lt;p class="caption"&gt;Multi-class confusion matrix for CoNNOR. When an image has multiple labels, it is considered separately for each label; additional labels associated with the image are excluded from the calculation of incorrect predictions. The equal-error rate for each class (computed from the ROC curve on the previous slide) is used as a cutoff threshold (e.g. different classes have different thresholds. Most classes achieve greater than 75% prediction accuracy. The model predicts quadrilaterals with higher frequency than supported by the data for all classes.&lt;/p&gt;
&lt;/div&gt;

.move-margin[

&lt;br/&gt;
For multi-label images, only incorrect predictions contribute to off-diagonal probabilities

`\(P &gt; EER_i\)` used as the cutoff
&lt;br/&gt;

]

???

Slightly more likely to predict quadrilateral for all labels. Two things are important to note here - first, "reality" and the image labels are different (due to imperfect labeling), and second, what label should be assigned to a specific image is ambiguous. We're working on correcting the mislabeling - some of it is due to the fact that the guidelines for labeling have changed since we started this mid-June. There's a certain amount of ambiguity in the geometric shapes in any case - strictly speaking, any amount of roundness should mean something isn't a quadrilateral, but what is it? Rubber typically doesn't create perfectly sharp corners. 

As with the ROC curves in the previous slide, this image is an adaptation of a confusion matrix, which is usually used to show the performance of a classification algorithm where each item is in only one class. Off-diagonal cells above the diagonal show incorrect classifications, while off-diagonal cells below the diagonal show missed classifications. To modify the confusion matrix for our problem, where an image can be labeled as any number of classes, we have excluded additional class labels when calculating the probability that something is misclassified. So if an image contains both a line and a circle, and the model labels it as containing a circle but not a line, when calculating the probability that a line image is misclassified as a circle, the image will be excluded (since it really should have been classified as a circle). 

---
class:primary
## Definitions matter
![Classes get confusing](labelme-imgs/dc_circle_quad_confusion.png)

--

![Not everything is labeled correctly](labelme-imgs/circle_pred_correct.png)

???

We created a shiny application to see the images and the model's predictions. Blue means that the image had that label, grey means it does not. 

So in the first image, the design is labeled as a quadrilateral (rounded quadrilateral) rather than a circle. We're still debating how to handle these... 

In the second image, both text and quad are labels, but the model also identifies a circle in the text with probability 0.31; that is, the text contains an O. 

We use the shiny application to screen for these problems so that we can correct the labeled training data. We're trying to ensure that the data used to train the model is of very high quality, while not spending millions of dollars to hire workers online to label things. Because we determined the guidelines for labeling the data, labeled the data (or oversaw the labeling), and trained the model ourselves, we have the advantage of knowing the flaws at every point in the process; that means we have the responsibility to fix those flaws where possible. We're not doing inference on the model results at this point (nor planning to use the data we're training the model with during the operational stage) so the data -&gt; model -&gt; fix data loop is less of a validity concern. 

When the model is sufficiently well-calibrated, we can then work with engineers to build the device, collect some initial data, and tweak the model weights with new data that better represents what we'll actually see from the collection equipment. By that point, hopefully we'll also have narrowed down the geometric classification scheme so that categories that are now somewhat fuzzy are more clearly operationalized.

---
class:primary
## Debugging the model

- Active area of research: How to debug, interpret, and understand what a CNN is actually doing    
.small[https://distill.pub/2018/building-blocks/]

- Which regions in the image are relevant to the class?

- Which filters are most important for detection of each class?





---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - DC](heatmaps/heatmap-quad-4-dc-pure-se-navy_product_7270757_color_9.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - adidas](heatmaps/heatmap-text-1-adidas-kids-adilette-clf-adj-toddler-little-kid-big-kid-black-white_product_8987203_color_151.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]

---
class:primary
## Class Activation Maps

.left-column[&lt;br/&gt;&lt;br/&gt;For each class, the heatmap is scaled so that the highest value is highlighted in yellow ]
.right-column[
![unscaled heatmapp - seychelles](heatmaps/heatmap-text-2-seychelles-slow-down-blush-metallic_product_9017725_color_34700.png)
]

.move-margin[&lt;br/&gt;&lt;br/&gt;Blue: Prediction matches image label &lt;br/&gt;&lt;br/&gt;Grey: Prediction does not match image label]


---
class:inverse
# &lt;br/&gt;&lt;br/&gt;What's Next?

---
class:primary
## Debugging the model
- Port the `keras-vis` Python library to R

    - Activation Maximization maps: generate a new image to maximize filter output activations
        - Show what specific filters are doing and "seeing"
        
    - What parts of an image are most important in activating a specific class?
        - Saliency Maps: plot gradients w.r.t. output
        - Class Activation Maps: plot gradients w.r.t. last conv. layer (maintains spatial information)
        
&lt;!-- Insert Jason's stuff here --&gt;        

---
class: primary
## Whole-Shoe Predictions
- Currently, predictions are for 256 x 256 chunks

- Integrate multiple chunks to provide geometric features with probabilities and relative coordinates

- Add spatial information to further discriminate between shoe models

&lt;!-- --- --&gt;
&lt;!-- class: primary --&gt;
&lt;!-- ## Uncertainty estimation --&gt;
&lt;!-- &lt;br/&gt; --&gt;
&lt;!-- - Confidence/Prediction/Credible intervals for Neural Network predictions and parameters are not common --&gt;
&lt;!--     - Conformal prediction --&gt;
&lt;!--     - [Bayes by Backprop](https://medium.com/neuralspace/bayesian-convolutional-neural-networks-with-bayes-by-backprop-c84dcaaf086e) - Bayesian frameworks for CNNs [Shr+18] --&gt;


---
class: primary
## Collect local population data

&lt;br/&gt;
- Build the machine (or rather, have engineers build it...)
- Collect data from (very) local areas
    - date/time stamp
    - side-view pictures of the shoes (where possible)
    - scans of the bottom of the shoe
- Label geometric features on the collected scans
- Run a 2nd stage of model weight optimization with scanned data
- Use this optimized model to analyze newly collected data

---
class: primary
## Local population data

- Use spatial locations + label probabilities as a feature vector

- Database of shoes in local population can be used to assess likelihood of a specific feature set occurring at random
    - Helpful to have error estimates for CNN output probabilities 
        - Bayesian CNNs [Shr+18]
    - Useful to know wear pattern frequency
        - As shoes wear, geometric features become less pronounced
        - Wear patterns can also be used as a class characteristic
        


---
class: inv-center
# Questions?

---
class: primary
## References
&lt;div class="small"&gt;

&lt;p&gt;[1]&lt;cite&gt;
I. Benedict, E. Corke, R. Morgan-Smith, et al.
&amp;ldquo;Geographical variation of shoeprint comparison class correspondences&amp;rdquo;.
In: &lt;em&gt;Science and Justice&lt;/em&gt; 54.5 (2014), pp. 335&amp;ndash;337.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[2]&lt;cite&gt;
F. Chollet and J. J. Allaire.
&lt;em&gt;Deep Learning with R&lt;/em&gt;.
Manning Publications Company, 2018.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[3]&lt;cite&gt;
S. Gross, D. Jeppesen and C. Neumann.
&amp;ldquo;The variability and significance of class characteristics in footwear impressions&amp;rdquo;.
In: &lt;em&gt;Journal of Forensic Identification&lt;/em&gt; 63.3 (2013), p. 332.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[4]&lt;cite&gt;
E. D. Hamm.
&amp;ldquo;The individuality of class characteristics in Converse All-Star footwear&amp;rdquo;.
In: &lt;em&gt;Journal of Forensic Identification&lt;/em&gt; 39.5 (1989), pp. 277&amp;ndash;292.&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[5]&lt;cite&gt;
A. Krizhevsky, I. Sutskever and G. E. Hinton.
&amp;ldquo;ImageNet Classification with Deep Convolutional Neural Networks&amp;rdquo;.
In: 
&lt;em&gt;Advances in Neural Information Processing Systems 25&lt;/em&gt;.
Ed. by F. Pereira, C. J. C. Burges, L. Bottou and K. Q. Weinberger.
Curran Associates, Inc., 2012, pp. 1097&amp;ndash;1105.
(Visited on 11/27/2018).&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[6]&lt;cite&gt;
K. Shridhar, F. Laumann, A. L. Maurin, et al.
&amp;ldquo;Bayesian Convolutional Neural Networks with Variational Inference&amp;rdquo;.
In: &lt;em&gt;arXiv:1806.05978 [cs, stat]&lt;/em&gt; (Jun. 2018).
(Visited on 01/17/2019).&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;[7]&lt;cite&gt;
K. Simonyan and A. Zisserman.
&amp;ldquo;Very Deep Convolutional Networks for Large-Scale Image Recognition&amp;rdquo;.
In: &lt;em&gt;arXiv:1409.1556 [cs]&lt;/em&gt; (Sep. 2014).
(Visited on 01/18/2019).&lt;/cite&gt;&lt;/p&gt;
&lt;/div&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
