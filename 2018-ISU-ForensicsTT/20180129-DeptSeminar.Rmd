---
title: "CoNNOR: Convolutional Neural Network for Outsole Recognition"
output:
  xaringan::moon_reader:
    css: ["default", "css/csafe.css", "css/csafe-fonts.css"]
    lib_dir: libs
    nature:
      countIncrementalSlides: false
---
```{r, echo = F, cache = F, include = F}
library(magrittr)
library(tidyverse)
library(ggplot2)

knitr::opts_chunk$set(echo = FALSE, message = F, warning = F, cache = T)
```
```{r, load_refs, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE, 
           bib.style = "authoryear", 
           cite.style = 'alphabetic', 
           style = "markdown",
           hyperlink = FALSE, 
           dashed = FALSE)
bb <- ReadBib("./refs.bib", check = FALSE)
```

```{r}
# Unicode symbols for classes
unicode_symbols <- c(bowtie = "",
  star = "\u2605",
  polygon = "\u2B23",
  chevron = "\uFE3D",
  triangle = "\u25BC",
  other = "\u003F",
  circle =  "\u23FA",
  text = "a",
  quad = "\u25A0",
  line = "\u2225"
  )
unicode_symbols <- data_frame(labels = names(unicode_symbols),
                              shape = as.character(unicode_symbols))
```

class:primary
## Outline

<br/>
- Defining the Problem
- Identifying features in shoes
- Convolutional Neural Networks
- Future Work

---
class:inverse
<h1><br/>What is the probability I made this shoeprint?</h1>

---
class:primary
## What is the probability I made this shoeprint?

1. Define the comparison population
2. Obtain data on the distribution of shoes in the comparison population
3. Identify shoes in the comparison population which could have produced the shoeprint
4. Establish whether I have any matching shoes

---
class:primary
## Comparison Population

.right-column[

<br/>
Geographic Area: 

- How local is local?

Time: 
- weekday, weekend
- morning, evening, overnight
- season

]

.left-column[

<img src="Snedecor-3105.png" width="100%" style="vertical-align:middle"/>

<img src="ISU-map.png" width="100%" style="vertical-align:middle"/>

<img src="Iowa-map.png" width="100%" style="vertical-align:middle"/>

<img src="US-map.png" width="100%" style="vertical-align:middle"/>

]
---
class:primary
## Comparison Population

<br/>
> .large[Quantifying the frequency of shoes in a local population is an unsolveable problem]<br/> - Leslie Hammer, March 2018


---
class:primary
## Comparison Population Shoes

- No 100% complete database of all shoes (manufacturer, model, size, tread molds)
    <!-- - Too many manufacturers, import/export, online sales, custom shoes, knockoffs -->
    <!-- - New shoes released all the time -->
- Shoe purchase data vs. frequency of wear
    <!-- - shoes purchased outside the defined geographic area or from non-participating retailers -->
    <!-- - frequency of wear -->
- Local populations may differ wildly
    <!-- - Styles and trends -->
    <!-- - Occupational differences -->
    <!-- - Retailer options -->

<br/><br/>
.center[<img src = "snow-boots.jpg" width = "50%" style = "vertical-align:middle;float:middle"/>]
<!-- https://pixnio.com/free-images/2017/05/03/2017-05-03-07-35-18-900x456.jpg -->

---
class:primary
## Comparison Population Shoes

How to collect data from the local population? 

- Low profile scanner that can be placed in a high traffic area
- Scan shoes of those walking past
- Create a local-area database of relevant shoes

--

### .center[This is an engineering problem]

---
class:primary
## Comparison Population Shoes

Assume a machine exists that can scan shoe outsoles of pedestrians

--

1. Identify relevant features within the images

--

2. Assess the frequency of similar shoes in local population data

--

### .center[These are statistics problems]

---
class:primary
## Identify Relevant Features

<br/>
Class Characteristics
- Make, Model, Tread pattern, Size, Type of shoe
- Cannot be used to identify an individual match
- Used for exclusion

---
class:primary
## Identify Relevant Features

<br/>
Use features other than make/model to characterize shoes

- Knockoffs often have very similar tread patterns
- Similar styles have similar tread patterns across brands
- Unknown shoes can still be classified and assessed

<img src = "star_bar_drmartin.png" width = "16%" style = "vertical-align:middle;padding:30px"/>
<img src = "star_bar_vibram.png" width = "15%" style = "vertical-align:middle;padding:30px"/>
<img src = "star_bar_timberland.png" width = "16%" style = "vertical-align:middle;padding:30px"/>

---
class:primary
## Identify Relevant Features

<br/>
Geometric Elements in Shoe Tread
- One type of class characteristic
- Can be used to narrow shoe prints down to make/model `r Citep(bb, 'gross_variability_2013')`

| Bowtie | Chevron | Circle |
| ------ | ------- | ------ |
| ![Bowtie examples](class_examples/bowtie_examples.png) | ![Chevron examples](class_examples/chevron_examples.png) | ![Circle examples](class_examples/circle_examples.png) |

| Line | Polygon | Quad |
| ---- | ------- | ---- |
| ![Line examples](class_examples/line_examples.png) | ![Polygon examples](class_examples/polygon_examples.png) | ![Quad examples](class_examples/quad_examples.png) |

| Star | Text | Triangle |
| ---- | ---- | -------- |
| ![Star examples](class_examples/star_examples.png) | ![text examples](class_examples/text_examples.png) | ![Triangle examples](class_examples/triangle_examples.png) |

---
class:primary
## Feature Detection

<br/>
Classic computer vision feature detection methods: 
- Edge, Corner, Blob, Ridge detection
- Template matching: Hough transforms for line, circle, ellipse detection
    - provide location and orientation

--

.pull-left[

Pros
- Algorithmic; no training data necessary

]

--

.pull-right[

Cons
- Parameters don't generalize well to different images
- Computationally intensive to process every image
- Features lack face validity

]

---
class:primary
## Feature Detection

<br/>
Convolutional neural networks: 
- Structure built to mimic perceptual pathways in the human visual system
- Ubiquitous in modern image recognition tasks

--

.pull-left[

Pros
- Pre-trained networks available for tuning    
.small[AlexNet, VGG16, ResNet, Inception]
- Features are interpretable
- Very fast (after training)

]

--

.pull-right[

Cons
- Requires labeled training data (lots!)
- Computationally intensive to train
- Opaque - parameters are not interpretable

]

---
class:inverse
# <br/><br/>Convolutional Neural Networks

---
class:primary
## Acquire Data

![Zappos Screenshot showing sole images](zappos.png)

.center[`r system("tree /home/srvander/Projects/CSAFE/ShoeScrapeR/extra/photos/ | tail -1", intern = T) %>% str_extract("\\d{1,} files") %>% str_replace("files", "shoe outsole images")` scraped since June 2018]

```{r labeled-data}
model_path <- "~/models/shoe_nn/RProcessedImages/"
data_dirs <- list.dirs(model_path, full.names = T, recursive = F) %>% rev()
newest_data_file <- NULL
startidx <- 1:10
while (is.null(newest_data_file) & max(startidx) <= length(data_dirs)) {
 newest_data_file <- data_dirs %>%
  magrittr::extract(startidx) %>%
  sapply(function(x) list.files(x, "cropped_photos.Rdata", recursive = F, full.names = T)) %>%
    unlist() %>%
    as.character() %>%
    magrittr::extract(1)

  startidx <- pmin(max(startidx) + 1:10, length(data_dirs))
}

load(newest_data_file)

label_fixes <- c("triangles" = "triangle",
                 "trianglee" = "triangle",
                 "circlemtraignle" = "circle_triangle",
                 "circle triangle" = "circle_triangle",
                 "circleline" = "circle_line",
                 "circle text" = "circle_text",
                 "circle_elongated" = "circle",
                 "cricle|cirle" = "circle",
                 "chrevron" = "chevron",
                 "lie" = "line",
                 "texxt" = "text",
                 "star quad" = "star_quad",
                 "exc_idd" = "exclude",
                 "quad?" = "quad",
                 "qaud|qud" = "quad", 
                 "stars|start" = "star", 
                 "exlude" = "exclude",
                 "rounded" = "",
                 "smooth_texture" = "other",
                 "octagon" = "polygon",
                 "crepe" = "other",
                 "hex" = "polygon",
                 "smooth" = "other"
                 )
# Merge doesn't work, wtf???
annotated_imgs <- select(dfunion, image, name) %>%
  mutate(base_image = basename(image)) %>%
  mutate(num_labels = str_count(name, "_") + 1, 
         annot_num = 1:n()) %>%
  mutate(labels = str_remove_all(name, "\\(.{1,2}\\)") %>% 
           str_replace_all(label_fixes) %>% 
           str_split("_")) %>%
  unnest(labels) %>%
  mutate(labels = str_replace_all(labels, label_fixes)) %>%
  mutate(label_type = ifelse(num_labels == 1, "single", "multi")) %>%
  filter(labels != "") %>%
  filter(labels != "exclude") %>%
  filter(labels != "ribbon" & labels != "logo")
```

---
class:primary
## Label Data

![LabelMe](LabelMe1.png)

.center[`r length(annotated_imgs$image)` shoe images labeled]
.center[`r length(unique(annotated_imgs$annot_num))` regions labeled with one or more geometric objects]
.center[Labeling courtesy of Jenny Kim, Ben Wonderlin, Holden Jud, and Mya Fisher]

---
class:primary
## Label Data
<br/>
```{r, fig.width = 5, fig.height = 3, out.width = "90%", dpi = 300}
ggplot(data = annotated_imgs) + 
  geom_bar(aes(x = labels, fill = label_type), color = "black") + 
  scale_fill_manual("Class\nLabels", values = c("single" = "#6ba2b9", "multi" = "#2e5597")) + 
  coord_flip() + 
  ylab("# Labeled Regions") + 
  xlab("") + 
  theme(legend.position = c(1, 0), legend.justification = c(1, 0), legend.background = element_rect(fill = "transparent")) + 
  ggtitle("Current Class Distribution (All Labeled Images)")
```

---
class:primary
## Train the CNN

<br/>
- VGG16 pre-trained network    
![VGG16 architecture](vgg16.png)

---
class:primary
## Train the CNN

<br/>
- VGG16 pre-trained network    
![VGG16 architecture](vgg16-tune.png)
- Tune parameters after the last convolutional layer; optimize for problem-specific input


```{r}
new_img_dir <- dirname(newest_data_file)

ann_df <- select(annotated_imgs, base_image, name, num_labels, annot_num, labels, label_type) %>%
  group_by(base_image, name) %>%
  arrange(annot_num) %>%
  mutate(ln = 1:n())

img_df <- data_frame(img = list.files(file.path(new_img_dir, c("test", "train", "validation")), full.names = T)) %>%
  mutate(type = str_extract(img, "(test|train|validation)"),
         image = basename(img),
         aug = grepl("^aug", image),
         name = str_extract(image, "^([a-z\\(\\)RE]*?_?){1,}-\\d{1,}-"),
         ln = str_extract(name, "-\\d{1,}-$") %>% gsub(pattern = "-", replacement = "") %>% as.numeric(),
         name = gsub(pattern = "-\\d{1,}-$", "", name),
         name = gsub(pattern = "aug_", "", name),
         name = gsub(pattern = "logo|ribbon|logo\\(R\\)|ribbon\\(R\\)", replacement = "other", name),
         base_image = gsub(image, pattern = "^([a-z\\(\\)RE]*?_?){1,}-\\d{1,}-", replacement = "") %>%
           gsub(., pattern = "_\\d_\\d{1,}.jpg", replacement = ".jpg")) %>%
  select(type, base_image, name, ln, aug, img_path = image) %>%
  mutate(num_labels = str_count(name, "_") + 1, 
         annot_num = 1:n()) %>%
  mutate(labels = str_remove_all(name, "\\(.{1,2}\\)") %>% 
           str_replace_all(label_fixes) %>% 
           str_split("_")) %>%
  unnest(labels) %>%
  mutate(labels = str_replace_all(labels, label_fixes)) %>%
  mutate(label_type = ifelse(num_labels == 1, "single", "multi")) %>%
  filter(labels != "" & labels != "hatching" & labels != "exclude" & !is.na(labels))
```

---
class:primary
## Train the CNN

<br/>
<div style="margin-left:-20px">
- 256 x 256 pixel images
- "One-hot" encoding - multiple classes are allowed for each image
- Training data (60%):
    - Augmented images (rotation, skew, zoom, crop) to prevent overfitting
    - Class weights used to counteract uneven class sizes
- Validation and test data (20% each)
</p>

---
class:primary
## Train the CNN

<br/>

```{r class-distribution, fig.width = 6, fig.height = 4, out.width = "90%", eval = F, include = F}
img_df %>%
  mutate(type = factor(type, levels = rev(c("train", "test", "validation")))) %>%
  group_by(labels, type) %>%
  mutate(labeln = sum(1/num_labels, na.rm = T)) %>%
  ungroup() %>%
  arrange(desc(labeln)) %>%
  mutate(labels = factor(labels, levels = unique(labels), ordered = T)) %>%
  group_by(type) %>%
  mutate(weight = 1/n()) %>%
  ungroup() %>%
ggplot() + 
  geom_bar(aes(x = type, fill = labels, weight = weight), color = "black", position = "stack") + 
  scale_fill_brewer("Class\nLabels", palette = "Paired") + 
  # facet_grid(~type) + 
  coord_flip() + 
  ylab("Proportion of Images") + 
  xlab("") + 
  theme(axis.text.y = element_text(angle = 90, hjust = .5, vjust = .5, size = 12)) + 
  ggtitle("Current Class Distribution (All Labeled Images)")
```

???

Goal is to increase the relative frequency of the rarer classes (and reduce "everything is a quad")

---
class:primary
## Train the CNN

<br/>
<img alt="Training accuracy increase" src="training.png" width = "90%"/>
```{r training-accuracy, fig.width = 7, fig.height = 5, out.width = "90%"}
library(keras)
model_dir <- file.path("~/models/shoe_nn/TrainedModels", str_extract(new_img_dir, "\\d{8}-\\d{6}$"))
load(list.files(model_dir, "-history.Rdata", full.names = T)[1])
data.frame(history$metrics) %>%
  mutate(epoch = 1:n()) %>%
  gather(key = "measure", value = "value", -epoch) %>%
  mutate(Type = ifelse(str_detect(measure, "val"), "Validation", "Training"),
         measure = ifelse(str_detect(measure, "acc"), "Accuracy", "Loss")) %>%
  ggplot(aes(x = epoch, y = value, color = Type)) + 
  geom_point() + 
  geom_smooth(se = F) + 
  facet_grid(measure~., scales = "free_y")
```


---
class:primary
## Train the CNN

.right-column[
<img src="ConfMat.png" alt = "Confusion Matrix" width = "95%"/>
]

.left-column[

<br/>
For multi-label images, only incorrect predictions contribute to off-diagonal probabilities
<br/>

]

???

Slightly more likely to predict quadrilateral for all classes. It's important to note that we don't expect perfect predictions here - there are some shapes that are actually super ambiguous (and were labeled differently -- we're working on correcting that issue)

---
class:primary
## Definitions matter...

<br/>
![Classes get confusing](dc_circle_quad_confusion.png)

--

![Not everything is labeled correctly](circle_pred_correct.png)

???

We created a shiny application to see the images and the model's predictions. Blue means that the image had that label, grey means it does not. 

So in the first image, the design is labeled as a quadrilateral (rounded quadrilateral) rather than a circle. We're still debating how to handle these... 

In the second image, both text and quad are labels, but the model also identifies a circle in the text with probability 0.31; that is, the text contains an O. 

We use the shiny application to screen for these problems so that we can correct the labeled training data. We're trying to ensure that the data used to train the model is of very high quality, while not spending millions of dollars to hire workers online to label things.

---
class:primary
## Debugging the model

- Active area of research: How to debug, interpret, and understand what a CNN is actually doing    
.small[https://distill.pub/2018/building-blocks/]


